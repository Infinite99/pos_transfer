{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1703\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_dev_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  12264\n",
      "Tagged words in test set: 185999\n",
      "****************************************\n",
      "Total sentences in dataset: 13700\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_dev_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 1.463 | Train Acc: 59.56%\n",
      "\t Val. Loss: 0.906 |  Val. Acc: 70.61%\n",
      "Epoch: 02 | Epoch Time: 0m 36s\n",
      "\tTrain Loss: 0.651 | Train Acc: 78.94%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 77.52%\n",
      "Epoch: 03 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.487 | Train Acc: 84.13%\n",
      "\t Val. Loss: 0.601 |  Val. Acc: 80.77%\n",
      "Epoch: 04 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.390 | Train Acc: 87.50%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 82.89%\n",
      "Epoch: 05 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.321 | Train Acc: 89.81%\n",
      "\t Val. Loss: 0.522 |  Val. Acc: 84.10%\n",
      "Epoch: 06 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.267 | Train Acc: 91.71%\n",
      "\t Val. Loss: 0.488 |  Val. Acc: 85.25%\n",
      "Epoch: 07 | Epoch Time: 0m 28s\n",
      "\tTrain Loss: 0.223 | Train Acc: 93.19%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 86.24%\n",
      "Epoch: 08 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.185 | Train Acc: 94.52%\n",
      "\t Val. Loss: 0.462 |  Val. Acc: 86.60%\n",
      "Epoch: 09 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.154 | Train Acc: 95.58%\n",
      "\t Val. Loss: 0.479 |  Val. Acc: 87.28%\n",
      "Epoch: 10 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.127 | Train Acc: 96.49%\n",
      "\t Val. Loss: 0.470 |  Val. Acc: 87.70%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.100 |  Test Acc: 97.46%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[ 0.2013,  1.2206, -1.4330,  ..., -1.0426, -0.6480, -1.2082],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.8961,  1.2473, -0.1504,  ..., -1.2013,  1.7057, -0.7254],\n",
      "        ...,\n",
      "        [ 1.5224, -0.4381,  0.4581,  ..., -1.6520, -0.6744, -0.9960],\n",
      "        [ 1.8362,  1.3754,  0.4620,  ...,  1.5907,  0.7552,  0.3885],\n",
      "        [ 0.8345,  0.2557,  0.9135,  ..., -1.5939, -0.9754, -0.7628]],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.0026, -0.0886,  0.0212,  ...,  0.0340, -0.0458, -0.0461],\n",
      "        [-0.1382, -0.0229,  0.0609,  ...,  0.1162,  0.0635,  0.1214],\n",
      "        [-0.0568,  0.1915, -0.0221,  ...,  0.0259, -0.0922, -0.0330],\n",
      "        ...,\n",
      "        [-0.0134, -0.0976,  0.0750,  ...,  0.1751,  0.0436,  0.0604],\n",
      "        [-0.0465,  0.1295,  0.0702,  ..., -0.1521,  0.0061,  0.1871],\n",
      "        [ 0.1288,  0.0266, -0.0111,  ...,  0.1379,  0.0181,  0.0512]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 2.8862e-01,  2.4862e-03, -1.1296e-01,  ...,  1.6856e-01,\n",
      "         -1.1175e-01, -9.6196e-02],\n",
      "        [ 1.0269e-01,  9.9104e-03, -1.6668e-01,  ...,  4.2459e-02,\n",
      "         -1.7012e-01, -1.8591e-01],\n",
      "        [ 8.1732e-02, -6.3241e-02, -1.3894e-02,  ...,  1.0509e-01,\n",
      "         -1.0484e-01, -7.8902e-02],\n",
      "        ...,\n",
      "        [-1.8527e-02, -9.1296e-02, -9.4271e-02,  ...,  4.5169e-05,\n",
      "         -4.1038e-02, -1.1884e-01],\n",
      "        [ 1.2913e-01, -1.1613e-01,  7.2274e-02,  ...,  1.5460e-01,\n",
      "         -2.3374e-02, -1.2955e-02],\n",
      "        [ 6.5472e-02,  1.2541e-01,  1.6593e-01,  ...,  5.0099e-03,\n",
      "          1.0931e-02, -3.5929e-02]], requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.0835,  0.0527,  0.1518,  0.1138,  0.1333, -0.0100,  0.1147,  0.1142,\n",
      "        -0.0076,  0.1261,  0.1426,  0.0215,  0.0879,  0.0601,  0.0537, -0.0082,\n",
      "         0.1056,  0.0054,  0.0897,  0.0070,  0.0661,  0.0249,  0.0956,  0.1209,\n",
      "         0.0366, -0.0091,  0.0233,  0.0032,  0.0049, -0.0252,  0.0109,  0.1665,\n",
      "         0.1224,  0.1183,  0.1373,  0.1208,  0.0818,  0.1434,  0.0637,  0.1312,\n",
      "        -0.0044,  0.0321,  0.1180,  0.1104,  0.0609, -0.0252,  0.0789,  0.0136,\n",
      "         0.1176,  0.0357,  0.0179,  0.0559,  0.0646,  0.0668, -0.0032, -0.0149,\n",
      "         0.0256,  0.0126,  0.0715,  0.0737,  0.0991, -0.0034,  0.0268,  0.0656,\n",
      "         0.0365,  0.0574,  0.1379,  0.1010,  0.0035,  0.1398,  0.1394,  0.0557,\n",
      "         0.0280,  0.1142,  0.1366, -0.0102,  0.0236, -0.0126,  0.0279,  0.0273,\n",
      "        -0.0286,  0.0813,  0.0615,  0.0619,  0.0763,  0.1560,  0.0716,  0.1578,\n",
      "         0.0875,  0.0304,  0.0159,  0.0331,  0.1423, -0.0108,  0.0829,  0.0042,\n",
      "         0.0033,  0.1040,  0.0934,  0.1456,  0.1054,  0.0219,  0.0670,  0.1400,\n",
      "         0.0091,  0.0864,  0.0492, -0.0062,  0.0185, -0.0014,  0.1125,  0.0922,\n",
      "         0.1320,  0.1157,  0.0342,  0.0705, -0.0082,  0.1194,  0.1388,  0.1045,\n",
      "         0.0781, -0.0264,  0.1426,  0.0451,  0.0128,  0.0551,  0.0717, -0.0204,\n",
      "         0.0692, -0.0470,  0.0391, -0.0226,  0.0500, -0.0706, -0.0714, -0.0297,\n",
      "         0.0075,  0.0547, -0.0657,  0.0532, -0.0117, -0.0110,  0.0598,  0.1314,\n",
      "         0.0519, -0.0023,  0.0780,  0.0415, -0.0216,  0.0405, -0.0652,  0.1326,\n",
      "         0.0445,  0.0303,  0.0233,  0.0142,  0.0259,  0.0771,  0.0962, -0.0261,\n",
      "         0.0581, -0.0119, -0.0330, -0.0073,  0.0184, -0.0239, -0.0212, -0.0914,\n",
      "        -0.0559, -0.0076, -0.0454,  0.0321,  0.0459,  0.0135,  0.0574,  0.0258,\n",
      "         0.0598,  0.0271,  0.0034,  0.0301,  0.0702,  0.0111, -0.0005, -0.0334,\n",
      "         0.0293, -0.0517,  0.0268,  0.0605,  0.0732, -0.0360, -0.0247,  0.0799,\n",
      "        -0.0610, -0.0058, -0.0977,  0.0293,  0.0300,  0.0725, -0.0579,  0.0511,\n",
      "        -0.0630, -0.0118,  0.0895,  0.0514,  0.0826,  0.0724, -0.1029,  0.0905,\n",
      "        -0.0380, -0.0865, -0.0630,  0.0554,  0.0792, -0.0393,  0.0213, -0.0366,\n",
      "        -0.0794, -0.0960,  0.0941,  0.0729,  0.0021,  0.0626, -0.0053, -0.0344,\n",
      "         0.0084,  0.0417,  0.0889, -0.0094,  0.1007,  0.0074, -0.0629, -0.0453,\n",
      "        -0.0022, -0.0215,  0.0055,  0.0044, -0.0655, -0.0207,  0.0724,  0.0127,\n",
      "        -0.1097,  0.0051, -0.0200, -0.0958,  0.0153, -0.0459, -0.1167,  0.1331,\n",
      "         0.0434, -0.0047,  0.0566,  0.0851, -0.0650, -0.0519, -0.0500,  0.1444,\n",
      "         0.0516, -0.0918,  0.0439, -0.0892,  0.0577,  0.0435,  0.0364,  0.0418,\n",
      "         0.0662, -0.0139, -0.0139, -0.0602, -0.0247, -0.0042, -0.0984, -0.0641,\n",
      "        -0.0787, -0.0166, -0.1268, -0.0774,  0.1328, -0.0871, -0.0332,  0.1224,\n",
      "        -0.0811, -0.0402, -0.0404,  0.0748,  0.0809,  0.1000,  0.0232,  0.0623,\n",
      "         0.0118, -0.0789,  0.0571, -0.1209,  0.1094, -0.0316,  0.0016,  0.0592,\n",
      "         0.0604, -0.1207,  0.1124,  0.0423,  0.0634,  0.1209,  0.0847, -0.0078,\n",
      "         0.0244, -0.0990, -0.1195, -0.0960, -0.0671,  0.0986,  0.0778,  0.0196,\n",
      "        -0.1131,  0.0985, -0.0117,  0.0459, -0.0746,  0.1067, -0.0718,  0.0348,\n",
      "         0.0787, -0.0122, -0.0633,  0.0306,  0.0706,  0.0247,  0.0779, -0.0593,\n",
      "         0.0074,  0.1276, -0.1188,  0.0385, -0.0015, -0.0349, -0.0065, -0.1268,\n",
      "        -0.1011, -0.0339, -0.0219,  0.1112,  0.0304,  0.0201, -0.0255,  0.0293,\n",
      "        -0.0665,  0.0717,  0.0885,  0.0139, -0.0365,  0.0371, -0.0593,  0.0425,\n",
      "         0.0907, -0.0833, -0.1438,  0.0637, -0.1302, -0.0054, -0.0259, -0.0234,\n",
      "         0.1226,  0.0099,  0.0153, -0.0905, -0.0807, -0.0849, -0.1063, -0.1220,\n",
      "        -0.0112, -0.0570, -0.0828, -0.0127, -0.0071, -0.0400,  0.0173, -0.0349,\n",
      "         0.0017,  0.1143, -0.1033, -0.0560, -0.0262,  0.0632, -0.1501, -0.0604,\n",
      "         0.0170,  0.0963,  0.0219,  0.0092,  0.0546,  0.0445, -0.0110,  0.1545,\n",
      "         0.1433,  0.1557,  0.0376,  0.0517,  0.0810,  0.0212,  0.0626,  0.1331,\n",
      "         0.1309,  0.1471,  0.0510,  0.1127,  0.1283,  0.0275,  0.1155, -0.0090,\n",
      "         0.1210,  0.1871,  0.0357,  0.0310,  0.1202,  0.0456,  0.0923,  0.0351,\n",
      "         0.1346,  0.1358,  0.1760,  0.1472,  0.0389,  0.0602,  0.0328,  0.0778,\n",
      "         0.0338, -0.0175,  0.0177,  0.1436,  0.1390,  0.1196,  0.0393,  0.1880,\n",
      "         0.1644,  0.0192,  0.1534,  0.0690,  0.0330,  0.0323,  0.1476,  0.0842,\n",
      "         0.0204,  0.1470,  0.0100,  0.0609,  0.0916,  0.1336,  0.1484,  0.0742,\n",
      "         0.0138,  0.0601,  0.0406,  0.0869,  0.0772,  0.1556,  0.1063,  0.1439,\n",
      "         0.0350,  0.1622,  0.0243,  0.0966,  0.0166,  0.0364,  0.1316,  0.1047,\n",
      "         0.0006,  0.1200,  0.0537,  0.1475,  0.0047,  0.0862,  0.0408,  0.0748,\n",
      "         0.1211,  0.0904,  0.0736,  0.0850,  0.1538,  0.1325,  0.0490,  0.0582,\n",
      "         0.1285,  0.1259,  0.0806,  0.1468,  0.0201,  0.0919,  0.0221,  0.0371,\n",
      "        -0.0093,  0.0094,  0.0282,  0.0076,  0.1479,  0.0935,  0.0427,  0.1670,\n",
      "         0.0621,  0.1291,  0.0578,  0.0142,  0.0067,  0.1447,  0.0464,  0.1680,\n",
      "         0.0051,  0.1310,  0.1606,  0.1290,  0.0740,  0.1595,  0.1214,  0.0551],\n",
      "       requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.0338,  0.1030,  0.0715,  0.0889,  0.0968,  0.0883,  0.1749,  0.0079,\n",
      "         0.0125,  0.0409,  0.0989, -0.0151,  0.1560, -0.0055,  0.0206,  0.0071,\n",
      "         0.0055,  0.0059,  0.0370,  0.1287,  0.0342, -0.0060,  0.0070,  0.1210,\n",
      "         0.0801,  0.1109,  0.1293,  0.1273,  0.0295,  0.1417,  0.0009,  0.1391,\n",
      "         0.1222, -0.0279,  0.1455,  0.0532,  0.0799,  0.0149,  0.1761,  0.0494,\n",
      "         0.0285,  0.0526,  0.1051,  0.1163,  0.0054, -0.0075,  0.0652,  0.1706,\n",
      "         0.0650,  0.1342,  0.1349,  0.1317,  0.1336,  0.0664,  0.1217, -0.0242,\n",
      "         0.1520,  0.1675,  0.1109,  0.0659,  0.1350,  0.0808,  0.1048,  0.1552,\n",
      "         0.0076,  0.0894,  0.0692,  0.0846, -0.0052,  0.1565,  0.1390, -0.0153,\n",
      "         0.0175,  0.0157,  0.0671,  0.0697,  0.0595, -0.0107,  0.0641,  0.1362,\n",
      "         0.1363,  0.0657,  0.1434,  0.0098,  0.0961,  0.0342,  0.1544,  0.1399,\n",
      "         0.0388,  0.1491,  0.0039,  0.0866,  0.0817,  0.0329,  0.0937,  0.1390,\n",
      "         0.1591,  0.0660,  0.0046,  0.1491,  0.0245,  0.0245,  0.1422,  0.1034,\n",
      "         0.1429,  0.1331,  0.0548,  0.0680,  0.1184,  0.0689,  0.0262,  0.0524,\n",
      "         0.1718,  0.1647,  0.0073,  0.1168,  0.1419,  0.0262,  0.1265,  0.1328,\n",
      "         0.0192,  0.0643,  0.1491,  0.0454,  0.0246,  0.0887,  0.0063,  0.1075,\n",
      "         0.1098,  0.0941, -0.0715,  0.0652, -0.0345, -0.0480, -0.0243, -0.0565,\n",
      "        -0.0333, -0.0956,  0.0956, -0.0267, -0.0079, -0.0845, -0.0723,  0.0380,\n",
      "        -0.0525,  0.0629,  0.0115, -0.0311,  0.0716, -0.0340,  0.0579,  0.0973,\n",
      "         0.0674,  0.0422,  0.0072,  0.0369, -0.0125,  0.1413,  0.0807,  0.0228,\n",
      "         0.0794,  0.0203, -0.0391,  0.0175,  0.1331,  0.0147,  0.0871,  0.0114,\n",
      "         0.0991,  0.1289,  0.0196, -0.0144, -0.0222, -0.0250, -0.0674,  0.0005,\n",
      "         0.0350,  0.0260,  0.0015,  0.0007,  0.0570, -0.0407,  0.1171, -0.0999,\n",
      "         0.0700,  0.0868,  0.0053,  0.1162, -0.0184, -0.0215, -0.0078,  0.0694,\n",
      "        -0.0110, -0.1017, -0.0977, -0.0027, -0.0029,  0.0820,  0.0530,  0.0346,\n",
      "        -0.1032, -0.0665, -0.0215, -0.0223, -0.0519,  0.0488, -0.1178,  0.0352,\n",
      "         0.0635,  0.0841, -0.0690,  0.0558, -0.0499, -0.0072,  0.0044, -0.0729,\n",
      "         0.0005, -0.0318,  0.0021,  0.0734,  0.0261, -0.0089,  0.0873,  0.0720,\n",
      "         0.0053, -0.0062,  0.0255, -0.0415, -0.0404, -0.0409,  0.0189, -0.0425,\n",
      "        -0.0291, -0.0041,  0.0328,  0.0111, -0.0014, -0.0734,  0.0461,  0.0234,\n",
      "        -0.0361, -0.0290, -0.0059, -0.0777, -0.0263, -0.0884,  0.0290,  0.0955,\n",
      "        -0.0853, -0.0428,  0.1165, -0.0165, -0.0272, -0.0556, -0.0327, -0.0142,\n",
      "         0.0593, -0.1207, -0.0480, -0.0538,  0.0545,  0.0724,  0.0336,  0.0761,\n",
      "         0.0734,  0.0602, -0.0842,  0.0129,  0.0947, -0.0128, -0.0692, -0.0143,\n",
      "        -0.0967,  0.0916, -0.1114, -0.0329,  0.0738,  0.0088, -0.0925,  0.0206,\n",
      "        -0.0256, -0.1092, -0.0507,  0.0315,  0.0772,  0.1113, -0.0557,  0.0296,\n",
      "         0.1052, -0.0227,  0.0734,  0.0021,  0.0555, -0.0765,  0.1326, -0.0438,\n",
      "         0.0073, -0.0116,  0.0934,  0.1021,  0.0991,  0.1078,  0.0219, -0.1116,\n",
      "         0.0427, -0.0049, -0.0499, -0.1291,  0.0675, -0.0213,  0.0925, -0.0965,\n",
      "        -0.0935,  0.0519, -0.0699, -0.0290, -0.0211,  0.0140,  0.0507,  0.1196,\n",
      "         0.0174, -0.0380, -0.1023,  0.1316,  0.0381, -0.1039,  0.0924, -0.0770,\n",
      "        -0.0871,  0.0027, -0.0656,  0.0430, -0.0932, -0.0379, -0.0129, -0.0687,\n",
      "        -0.1133,  0.0583, -0.0674,  0.0918, -0.0292,  0.0765,  0.1011,  0.0612,\n",
      "        -0.0844,  0.0398,  0.0998, -0.0629, -0.0502,  0.0037, -0.1101, -0.0289,\n",
      "        -0.0522, -0.1182, -0.0627,  0.1209, -0.1272,  0.0656, -0.0028, -0.1078,\n",
      "         0.0324, -0.1493, -0.0088, -0.1026, -0.0489, -0.0097, -0.0472, -0.1155,\n",
      "         0.1116, -0.0768, -0.0651,  0.0599, -0.0472, -0.0653,  0.0722, -0.0859,\n",
      "        -0.0703,  0.0535, -0.0363, -0.0847, -0.0010,  0.0885, -0.0235, -0.1094,\n",
      "         0.1421,  0.1364,  0.1424,  0.0151,  0.0238,  0.0678,  0.0525,  0.1557,\n",
      "         0.0239,  0.0724,  0.0346,  0.0933,  0.1287,  0.1582,  0.0526,  0.1065,\n",
      "         0.0136,  0.0756,  0.1355,  0.1310,  0.0642,  0.1612,  0.1360,  0.0759,\n",
      "         0.1533,  0.1368,  0.0468,  0.1143,  0.1104,  0.0580,  0.1201,  0.0085,\n",
      "         0.1769,  0.1237,  0.1152,  0.1402,  0.1717,  0.1029,  0.0344,  0.0088,\n",
      "         0.0816,  0.0006,  0.1338,  0.1757, -0.0158,  0.0503,  0.1313,  0.0442,\n",
      "         0.1440,  0.1266,  0.0862,  0.1036,  0.0748,  0.1554,  0.0902,  0.1611,\n",
      "         0.1255,  0.1214,  0.1046,  0.0966,  0.0364,  0.1364,  0.1268,  0.1625,\n",
      "         0.1414,  0.0918,  0.1602,  0.0230,  0.0437,  0.0059,  0.1092,  0.1161,\n",
      "         0.0771,  0.1088,  0.0670,  0.1314,  0.0248,  0.1748,  0.0663,  0.0925,\n",
      "         0.0947,  0.0656,  0.1036,  0.1090,  0.1366,  0.1264,  0.0014,  0.0125,\n",
      "         0.0230,  0.0360,  0.0709,  0.0982,  0.0438, -0.0238,  0.1238,  0.0634,\n",
      "         0.1148,  0.0966,  0.1743,  0.0480,  0.1144, -0.0034,  0.1444,  0.0492,\n",
      "        -0.0103,  0.1309,  0.1219,  0.1515,  0.1111,  0.0331,  0.0686,  0.0846,\n",
      "         0.1352,  0.0643,  0.1264,  0.0842,  0.0673,  0.0528,  0.1291,  0.0016,\n",
      "         0.1442,  0.0014,  0.1663,  0.1579,  0.1158,  0.0825,  0.1581,  0.0582],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[-0.1679, -0.0248, -0.0701,  ..., -0.1562, -0.0684,  0.1353],\n",
      "        [-0.0639, -0.1424, -0.0290,  ...,  0.1481, -0.0509, -0.0592],\n",
      "        [-0.0517,  0.1419, -0.0308,  ..., -0.0260,  0.0833, -0.0260],\n",
      "        ...,\n",
      "        [-0.0692,  0.0424, -0.0251,  ...,  0.0518, -0.1258,  0.0645],\n",
      "        [ 0.2198, -0.1282, -0.0083,  ...,  0.1268, -0.2134, -0.1278],\n",
      "        [ 0.0402,  0.0151,  0.0073,  ...,  0.1110,  0.0597,  0.0435]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[-1.3887e-01, -1.2050e-02, -1.7897e-02,  ...,  2.5674e-03,\n",
      "          2.3279e-03, -5.3065e-02],\n",
      "        [-1.1668e-01, -5.3812e-02, -5.5970e-02,  ...,  7.1374e-02,\n",
      "         -1.0495e-01, -6.5821e-02],\n",
      "        [-1.6105e-01, -4.7548e-03, -7.8601e-02,  ...,  1.5734e-02,\n",
      "         -4.4172e-02, -6.9975e-02],\n",
      "        ...,\n",
      "        [ 2.6701e-02, -1.6599e-02, -1.5980e-01,  ..., -4.7835e-02,\n",
      "         -1.4635e-01, -5.7853e-02],\n",
      "        [-5.9504e-02, -5.5621e-02, -3.0760e-02,  ...,  6.1954e-02,\n",
      "         -3.3767e-02, -6.3256e-05],\n",
      "        [-1.2895e-01,  1.9906e-02, -1.4212e-01,  ...,  8.4004e-02,\n",
      "         -1.3222e-01,  3.7699e-02]], requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([ 1.3332e-01,  9.0302e-02,  4.0393e-02,  7.5693e-02,  3.2371e-02,\n",
      "         1.2905e-01,  3.3536e-02,  1.3239e-01,  8.6708e-02,  5.2002e-02,\n",
      "         5.4252e-02,  1.2180e-01,  7.4743e-02,  1.1250e-01,  1.6767e-01,\n",
      "        -3.9518e-03,  1.7284e-01, -2.7228e-02,  9.1167e-02,  4.6133e-02,\n",
      "         3.9951e-02,  1.0271e-01, -2.8791e-02,  8.5640e-02,  1.1278e-01,\n",
      "         1.0250e-02,  1.2457e-01,  2.5918e-02,  1.2175e-02, -1.0741e-03,\n",
      "         8.9175e-02,  9.9037e-04,  1.3168e-01,  1.1895e-01,  1.4803e-01,\n",
      "         1.4011e-01,  7.9429e-02,  6.4058e-02,  1.2308e-01,  1.1817e-01,\n",
      "         1.2226e-01,  7.7053e-02,  4.7751e-02,  4.3670e-02,  9.5825e-02,\n",
      "         1.3114e-01,  4.2549e-02, -5.4414e-03,  2.3710e-02,  7.1761e-02,\n",
      "        -3.0596e-02,  1.2501e-01, -2.4809e-02, -1.8889e-02,  1.7869e-02,\n",
      "         3.1388e-02,  1.3575e-01,  5.5483e-02,  2.6294e-02,  1.3027e-01,\n",
      "         7.5089e-02,  1.1534e-01, -3.6066e-03, -5.8378e-03,  1.0605e-01,\n",
      "         6.3008e-02,  1.3109e-01,  9.0851e-02,  3.1937e-02, -1.0245e-02,\n",
      "         4.3651e-02, -2.0953e-03,  1.2219e-01,  1.2280e-01,  7.8020e-02,\n",
      "         3.2400e-02,  4.4087e-02,  4.2289e-02,  7.1258e-02,  1.3621e-01,\n",
      "         2.0491e-02,  2.0429e-02, -2.8286e-02,  9.8878e-02,  9.5888e-02,\n",
      "         8.3257e-02,  3.1365e-02,  1.5533e-01,  1.0482e-01,  1.2635e-01,\n",
      "         1.4235e-01, -4.2342e-02, -1.8217e-02,  1.0754e-01,  6.9179e-02,\n",
      "         1.3107e-01, -3.0673e-02, -2.7693e-02,  2.8389e-02,  1.1327e-02,\n",
      "         1.7814e-04,  1.0135e-01,  1.2532e-01, -1.2410e-02,  6.7689e-02,\n",
      "         1.6213e-01,  7.6417e-02,  2.6114e-04,  8.8054e-02,  1.1267e-01,\n",
      "         2.9842e-02,  1.1342e-01,  7.2701e-02,  7.5861e-02,  1.3841e-02,\n",
      "         6.5369e-03,  4.4990e-02, -4.6525e-03,  1.1890e-01,  4.9815e-02,\n",
      "         1.0637e-01,  6.2078e-02, -2.8777e-02,  6.2253e-02,  1.5065e-01,\n",
      "         1.0986e-01,  1.3590e-02,  3.8581e-02,  8.5776e-02, -1.0274e-01,\n",
      "        -9.6793e-02,  3.8590e-02, -2.2085e-02, -1.9543e-02,  7.8839e-02,\n",
      "        -7.9263e-02, -8.0415e-02, -1.3800e-02,  1.6475e-02,  4.7840e-02,\n",
      "         1.7777e-02,  7.5776e-02, -1.1808e-01,  3.2658e-02, -3.5970e-03,\n",
      "        -2.6509e-02,  1.1358e-01,  3.9433e-02,  7.9514e-02,  1.7101e-02,\n",
      "        -9.3895e-02,  2.2572e-02,  7.1652e-02,  6.8726e-02,  8.3542e-02,\n",
      "        -5.7513e-03,  8.5626e-02, -1.0538e-01,  9.9373e-02,  8.3292e-05,\n",
      "        -1.1565e-02, -7.9922e-02, -1.1343e-02, -4.3099e-03,  5.8140e-03,\n",
      "        -2.4639e-02,  1.0637e-01,  5.4899e-02,  1.1052e-02,  7.2901e-02,\n",
      "        -7.8290e-02,  7.5024e-02,  2.7252e-02,  1.8363e-02, -2.8221e-02,\n",
      "         8.9238e-02, -3.3168e-03, -3.1983e-02,  7.3548e-03, -4.0724e-02,\n",
      "         2.5803e-02,  8.4656e-02,  4.7362e-02, -5.4703e-02, -1.6476e-02,\n",
      "         5.3143e-02,  8.3664e-02,  7.5175e-02, -5.0878e-02, -3.8781e-02,\n",
      "        -2.7136e-02, -3.1693e-02,  5.6579e-02, -2.9396e-02,  3.1201e-02,\n",
      "         9.8405e-02,  1.4566e-02,  1.0327e-02, -5.7840e-02, -1.3326e-03,\n",
      "         2.5804e-02, -5.7837e-02, -3.4972e-02, -5.9403e-02,  6.8777e-02,\n",
      "         8.7436e-02, -1.1450e-01, -6.0789e-02,  1.2375e-01,  1.6004e-02,\n",
      "         3.8896e-02, -7.1749e-02,  6.4594e-02, -6.1677e-02, -4.3561e-02,\n",
      "         5.8105e-02, -6.2332e-02,  8.9397e-02, -5.7420e-02,  1.0153e-01,\n",
      "        -1.5124e-02, -4.6000e-02,  9.9344e-02, -8.2226e-02,  5.8307e-02,\n",
      "         1.9144e-02, -6.4311e-02, -2.8072e-02,  1.5586e-02, -6.2848e-03,\n",
      "         2.9813e-02, -3.1075e-03,  7.3947e-02, -7.8208e-02, -5.0554e-02,\n",
      "         5.0352e-02,  3.6208e-02, -3.3307e-02, -2.6158e-03,  1.1219e-03,\n",
      "         1.6789e-02,  3.8533e-02, -1.5912e-02,  3.6790e-02, -6.5137e-02,\n",
      "         1.0876e-01, -6.7540e-02,  3.7947e-02,  1.3512e-02,  3.2304e-02,\n",
      "         2.0664e-02, -7.2051e-02,  5.2712e-03, -3.0640e-02, -3.2700e-02,\n",
      "        -6.4388e-02, -7.7051e-02,  1.6109e-02, -6.8032e-02, -1.5895e-02,\n",
      "        -3.7480e-02, -2.9202e-03,  7.2523e-02, -1.1899e-01,  3.7208e-02,\n",
      "         1.6536e-02, -1.3947e-03, -1.3088e-01, -4.1569e-02, -1.1886e-01,\n",
      "         6.0676e-02, -7.4997e-03,  5.3896e-02, -7.5043e-02,  8.1990e-02,\n",
      "        -1.2849e-02, -1.1022e-02, -5.7154e-02, -1.0329e-02,  3.0260e-04,\n",
      "         8.1982e-02, -4.5089e-02,  1.0056e-01, -8.2373e-02, -7.7036e-02,\n",
      "         1.7551e-02, -4.1580e-02,  5.1790e-02, -9.9097e-02, -1.1325e-01,\n",
      "        -9.3290e-02, -1.2024e-01, -1.0362e-01,  7.5254e-02,  8.1231e-02,\n",
      "        -4.6487e-02, -5.8125e-02, -8.0185e-02, -1.2129e-01,  1.0228e-01,\n",
      "         3.9571e-02,  7.2453e-03, -2.7862e-02, -9.9094e-02, -7.7255e-02,\n",
      "        -7.3857e-02,  1.9291e-02,  1.0629e-01, -5.0775e-02, -8.2981e-02,\n",
      "         1.0695e-02, -1.1054e-01,  1.2842e-01, -1.1168e-01, -1.1882e-02,\n",
      "         7.9731e-02,  1.0141e-01,  1.1775e-01, -1.0509e-01, -6.9962e-02,\n",
      "         1.2779e-02,  9.6855e-02,  1.0770e-01,  1.0318e-01,  8.4463e-02,\n",
      "         8.7318e-03, -7.4206e-02, -3.8819e-02, -8.2712e-02, -4.5299e-02,\n",
      "        -5.6079e-02,  1.7528e-02, -5.4929e-02,  3.2918e-02,  3.0827e-02,\n",
      "        -1.2944e-01,  7.9032e-03,  1.0208e-01,  4.8865e-02, -9.7293e-04,\n",
      "        -1.8064e-02, -5.1337e-02, -1.4415e-02,  5.9123e-02,  1.2116e-02,\n",
      "        -1.0230e-01, -9.5080e-02,  1.0731e-01,  8.2376e-02, -4.4058e-02,\n",
      "        -7.6389e-02, -7.1351e-02, -9.4993e-02,  1.3279e-02, -5.9071e-02,\n",
      "        -3.3951e-02,  1.1444e-01, -3.1823e-03,  4.9610e-02,  1.0733e-01,\n",
      "         7.3872e-02,  2.6086e-02,  7.9665e-02,  1.5061e-02, -7.2387e-03,\n",
      "        -9.4934e-02, -8.9000e-02,  9.6301e-02, -1.3848e-02,  1.1111e-01,\n",
      "         8.1905e-02,  5.6464e-02, -4.0243e-02,  8.8626e-02,  6.9870e-02,\n",
      "         3.8544e-02,  9.0644e-02, -4.4771e-02, -3.0572e-02,  6.9173e-02,\n",
      "         1.1410e-01,  1.0505e-01, -1.1873e-01, -1.0155e-01,  2.8442e-02,\n",
      "         1.0572e-01,  1.0917e-01,  3.6216e-02,  1.3990e-02,  7.0415e-02,\n",
      "         1.1066e-01,  1.2636e-01, -7.6994e-03,  2.6228e-02,  3.1589e-02,\n",
      "         7.5522e-02,  1.7962e-01,  5.8082e-02, -1.8032e-02,  9.2335e-02,\n",
      "        -9.1933e-03,  1.3454e-01,  1.1366e-01, -6.1828e-03,  1.5699e-01,\n",
      "         1.1502e-01,  4.7823e-02,  3.5788e-02,  5.7057e-02,  8.2799e-02,\n",
      "         1.4834e-01,  2.1026e-02,  1.3148e-01, -9.1637e-03,  1.5137e-01,\n",
      "         4.4997e-02, -3.1511e-03,  6.0111e-02,  3.2017e-02,  4.4212e-02,\n",
      "        -1.2537e-03,  1.1886e-01,  4.7685e-03,  2.8029e-02, -5.0316e-03,\n",
      "         6.8310e-02,  9.3334e-02, -3.9154e-03,  9.9596e-02, -7.3062e-03,\n",
      "         6.4565e-02,  1.2110e-02,  9.5736e-02,  8.3473e-02,  7.5326e-04,\n",
      "         5.3568e-03,  1.3469e-01,  8.0765e-02, -1.3594e-02,  1.0247e-01,\n",
      "         1.9408e-02,  4.3249e-02, -4.0103e-02,  6.9774e-02,  6.7539e-02,\n",
      "         1.4444e-01,  4.8609e-02,  6.8518e-02,  1.1921e-01,  1.3949e-01,\n",
      "         5.6961e-02, -1.7269e-02,  1.7728e-02,  7.9839e-02,  1.4065e-02,\n",
      "         5.4981e-03,  6.0148e-02,  6.3058e-02,  4.6466e-02,  1.3623e-01,\n",
      "         9.9431e-02,  1.5156e-01,  2.8248e-02, -2.2008e-02,  2.5042e-02,\n",
      "         2.0167e-02,  1.3356e-02,  6.1319e-02,  4.0288e-03,  1.3968e-02,\n",
      "         1.0136e-01,  1.0671e-01,  6.4731e-02,  6.1113e-02, -1.0915e-02,\n",
      "         6.8061e-02,  1.1846e-02,  1.8140e-02,  3.3795e-02,  5.8710e-02,\n",
      "         9.3150e-03,  1.4829e-02,  6.0235e-02,  2.5712e-02,  9.8485e-02,\n",
      "         6.1109e-02,  1.1020e-01, -2.5296e-02,  9.1783e-02,  1.1312e-01,\n",
      "         5.6372e-02,  1.1112e-01,  7.8905e-02,  5.5836e-02,  6.9525e-02,\n",
      "        -8.0026e-03,  3.6974e-02,  1.3122e-01,  8.4813e-02,  1.9559e-02,\n",
      "         1.6614e-01,  2.7984e-02,  3.6842e-02,  4.0802e-02,  3.0352e-02,\n",
      "         8.9676e-02,  3.5495e-03,  1.3147e-01,  2.0676e-03, -2.4659e-02,\n",
      "        -2.4965e-02,  8.5627e-02], requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([ 0.0126,  0.0781, -0.0082, -0.0211,  0.0693, -0.0191, -0.0082,  0.0854,\n",
      "         0.0502,  0.0911,  0.0137,  0.1072,  0.0531,  0.0473,  0.0765,  0.0006,\n",
      "         0.1801,  0.0864,  0.0652, -0.0179,  0.0234,  0.0194,  0.0277,  0.0271,\n",
      "         0.0271,  0.0738, -0.0247,  0.1247,  0.0714,  0.0254,  0.1333, -0.0086,\n",
      "         0.0102,  0.1199,  0.0377,  0.0574,  0.0162,  0.1240, -0.0178,  0.0055,\n",
      "         0.1136, -0.0076,  0.0675, -0.0132,  0.0745, -0.0281, -0.0139,  0.1368,\n",
      "         0.0796,  0.0351,  0.0991,  0.1028,  0.0184,  0.0102,  0.0936,  0.0712,\n",
      "         0.0994,  0.0861,  0.1088,  0.0041,  0.0619,  0.0005,  0.1043, -0.0218,\n",
      "         0.1618,  0.0925, -0.0127, -0.0201,  0.1212,  0.1272,  0.0954,  0.0872,\n",
      "         0.0219,  0.0396,  0.0315,  0.0750, -0.0293,  0.0026,  0.0592,  0.1240,\n",
      "         0.0067,  0.0748,  0.0023,  0.1600,  0.0795,  0.0633,  0.0220,  0.0278,\n",
      "         0.0475,  0.0128,  0.0580, -0.0278,  0.0774,  0.0865,  0.1405,  0.1118,\n",
      "         0.0557,  0.0881, -0.0188,  0.0303, -0.0157,  0.0451,  0.0855,  0.1135,\n",
      "         0.0504, -0.0052,  0.0620,  0.1218,  0.0931,  0.0781,  0.1414, -0.0108,\n",
      "         0.0718,  0.0115,  0.0049, -0.0029,  0.0987,  0.0213,  0.1291,  0.0963,\n",
      "         0.0578, -0.0040,  0.0787,  0.0694,  0.0783, -0.0130,  0.0902,  0.0441,\n",
      "        -0.0292,  0.0407, -0.0679, -0.0387,  0.0113, -0.0549, -0.0087, -0.0706,\n",
      "        -0.0958,  0.0458, -0.0800,  0.0792, -0.0724, -0.0412,  0.0422,  0.1194,\n",
      "        -0.0693, -0.0092,  0.0141,  0.0753, -0.0235,  0.0413, -0.0880, -0.0202,\n",
      "        -0.0307,  0.0616,  0.0136, -0.0522, -0.0483, -0.0917,  0.0234,  0.0421,\n",
      "        -0.0411, -0.0755,  0.0488,  0.0297, -0.0323,  0.0902, -0.0134,  0.0680,\n",
      "        -0.0560,  0.0564, -0.0751, -0.0241, -0.0911,  0.0472,  0.0653,  0.0630,\n",
      "         0.0137,  0.0479, -0.0183, -0.0130, -0.0796,  0.0743, -0.0040, -0.0915,\n",
      "         0.0572, -0.0275, -0.0322,  0.0363, -0.0228,  0.0371, -0.0115, -0.1147,\n",
      "         0.0683, -0.0458,  0.0081,  0.0544,  0.0524,  0.0930,  0.0128, -0.0636,\n",
      "         0.0847,  0.0382,  0.0029, -0.0796, -0.0418, -0.0605, -0.0228,  0.0513,\n",
      "        -0.0453,  0.0348,  0.0230, -0.0064, -0.0726, -0.0017, -0.0421,  0.0519,\n",
      "        -0.0625,  0.0811,  0.0884,  0.0419,  0.0123,  0.0727,  0.0834, -0.0480,\n",
      "         0.0695, -0.0071,  0.0222, -0.0670,  0.1117, -0.0979, -0.0454, -0.0697,\n",
      "         0.0142, -0.0433,  0.0857, -0.0579,  0.0036, -0.0332,  0.0482,  0.1037,\n",
      "        -0.0009,  0.0488, -0.0014, -0.0648,  0.0230,  0.0362,  0.0364,  0.0610,\n",
      "        -0.0566,  0.0789, -0.0290, -0.0131, -0.0942,  0.0009,  0.0949,  0.0742,\n",
      "        -0.1134, -0.0062, -0.0526,  0.0531,  0.0323, -0.0861,  0.0896, -0.0500,\n",
      "        -0.0244,  0.1177,  0.0397, -0.0122,  0.1212, -0.0649, -0.0817, -0.0825,\n",
      "        -0.0246, -0.0468,  0.1169,  0.0278, -0.1275, -0.0857, -0.0627,  0.0105,\n",
      "        -0.0062, -0.0922,  0.0182, -0.0935, -0.0896,  0.0389, -0.0688, -0.1036,\n",
      "         0.0208,  0.0053, -0.0598, -0.0572, -0.0822,  0.0112,  0.0844,  0.0221,\n",
      "         0.0354, -0.0851, -0.0076,  0.0055,  0.0581,  0.0003, -0.0198,  0.0046,\n",
      "        -0.0175, -0.0246,  0.0946,  0.0840, -0.0891, -0.0241,  0.1284,  0.0247,\n",
      "        -0.0336, -0.1137, -0.0197,  0.0724,  0.0854, -0.0490, -0.0173,  0.0319,\n",
      "         0.1422,  0.0982,  0.0838,  0.0843,  0.0536,  0.0494,  0.0278, -0.0480,\n",
      "        -0.0882,  0.0067, -0.1115,  0.0100,  0.0720,  0.1347, -0.0488, -0.0185,\n",
      "        -0.0577,  0.0993,  0.0321, -0.0977,  0.0624, -0.1127,  0.0818,  0.0519,\n",
      "         0.0028, -0.1012, -0.1074,  0.1140, -0.0426, -0.0792, -0.0854, -0.0385,\n",
      "        -0.0249, -0.0413, -0.0928, -0.1023,  0.0470, -0.0689,  0.0431, -0.0547,\n",
      "        -0.0581, -0.0614,  0.0222,  0.0030, -0.0264, -0.0386, -0.0699,  0.1126,\n",
      "         0.0004,  0.0240,  0.0472,  0.0941, -0.0003,  0.0233, -0.0539, -0.0188,\n",
      "         0.0698,  0.0620, -0.0412, -0.0372,  0.0837,  0.0712, -0.1137, -0.0458,\n",
      "         0.1161,  0.0411,  0.0818,  0.0147,  0.1100,  0.1196, -0.0306,  0.0877,\n",
      "         0.0688,  0.0398,  0.1417,  0.1473,  0.1391,  0.0189,  0.0138,  0.0779,\n",
      "         0.0981,  0.1018, -0.0037,  0.0730,  0.1597, -0.0035, -0.0070,  0.1429,\n",
      "         0.0723,  0.0303, -0.0113, -0.0108,  0.1127,  0.0382,  0.0798,  0.0153,\n",
      "         0.0673,  0.0923,  0.1573,  0.0214,  0.0434,  0.1105,  0.0147, -0.0050,\n",
      "         0.0861,  0.0310,  0.0990,  0.1245,  0.0483,  0.0496,  0.0004,  0.0129,\n",
      "         0.0636,  0.0979, -0.0023, -0.0032,  0.0400,  0.0295,  0.1507,  0.0419,\n",
      "         0.1200, -0.0039, -0.0275, -0.0310,  0.0673,  0.0089,  0.0562,  0.0007,\n",
      "         0.1292, -0.0217,  0.1081,  0.0638,  0.0254,  0.1518,  0.0363,  0.0458,\n",
      "         0.0837,  0.0634,  0.1217,  0.0353,  0.1340,  0.1069,  0.1646,  0.1353,\n",
      "         0.0783,  0.0371,  0.0410,  0.0886,  0.1262,  0.0207,  0.1370, -0.0116,\n",
      "         0.1032,  0.0867,  0.1047,  0.1053,  0.1720,  0.0260,  0.0952,  0.0789,\n",
      "         0.1288,  0.0784, -0.0230,  0.0585, -0.0115,  0.1088,  0.0878, -0.0298,\n",
      "         0.0223,  0.0692, -0.0130,  0.0190, -0.0042,  0.0610,  0.0993,  0.0894,\n",
      "         0.0358,  0.0856,  0.0167,  0.0086,  0.0131,  0.1253,  0.1439,  0.0538,\n",
      "         0.1165,  0.0709,  0.0665,  0.1628, -0.0078, -0.0150,  0.1313,  0.0839],\n",
      "       requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[ 0.0031,  0.0207,  0.0879,  ..., -0.1011,  0.0446,  0.0829],\n",
      "        [-0.0983,  0.0516,  0.0363,  ..., -0.0745,  0.1020,  0.0043],\n",
      "        [-0.0163,  0.0860,  0.0687,  ...,  0.0918,  0.0876,  0.1200],\n",
      "        ...,\n",
      "        [ 0.0482, -0.0082, -0.0477,  ...,  0.0370, -0.0825,  0.0148],\n",
      "        [ 0.0658,  0.0226, -0.0299,  ..., -0.0529, -0.0015,  0.1062],\n",
      "        [ 0.1231,  0.0323,  0.0860,  ..., -0.0881, -0.0148, -0.0386]],\n",
      "       requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([ 0.0231,  0.0154,  0.0814, -0.0438, -0.0267,  0.0356,  0.0267, -0.0380,\n",
      "        -0.0033, -0.0360,  0.0264,  0.0710,  0.0358,  0.0333, -0.0099,  0.0454,\n",
      "        -0.0147, -0.0312, -0.0301,  0.0078], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 2.465 | Train Acc: 29.22%\n",
      "\t Val. Loss: 2.083 |  Val. Acc: 42.51%\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.782 | Train Acc: 56.94%\n",
      "\t Val. Loss: 1.627 |  Val. Acc: 58.42%\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.381 | Train Acc: 65.85%\n",
      "\t Val. Loss: 1.322 |  Val. Acc: 66.81%\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.108 | Train Acc: 72.97%\n",
      "\t Val. Loss: 1.105 |  Val. Acc: 72.25%\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.914 | Train Acc: 76.48%\n",
      "\t Val. Loss: 0.949 |  Val. Acc: 75.02%\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.780 | Train Acc: 79.06%\n",
      "\t Val. Loss: 0.839 |  Val. Acc: 76.93%\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.681 | Train Acc: 81.47%\n",
      "\t Val. Loss: 0.760 |  Val. Acc: 79.14%\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.602 | Train Acc: 83.74%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 80.65%\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.537 | Train Acc: 85.49%\n",
      "\t Val. Loss: 0.652 |  Val. Acc: 81.56%\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.486 | Train Acc: 87.14%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 82.65%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.568 |  Test Acc: 84.12%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
