{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torchtext\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_vec = torchtext.vocab.FastText(language='af')\n",
    "du_vec = torchtext.vocab.FastText(language='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1703\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_dev_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  12264\n",
      "Tagged words in test set: 185999\n",
      "****************************************\n",
      "Total sentences in dataset: 13700\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_dev_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans words missing:  0\n",
      "dutch words missing:  0\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "afr_matrix_len = len(train_afr.vocab.itos)\n",
    "afr_weights_matrix = torch.zeros((afr_matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        afr_weights_matrix[i] = af_vec[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        afr_weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "        words_missing += 1\n",
    "\n",
    "print(\"Afrikaans words missing: \", words_missing)\n",
    "\n",
    "du_matrix_len = len(train_du.vocab.itos)\n",
    "du_weights_matrix = torch.zeros((du_matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        du_weights_matrix[i] = du_vec[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        du_weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "        words_missing += 1\n",
    "\n",
    "print(\"dutch words missing: \", words_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, pad_idx, non_trainable=False):\n",
    "    input_dim, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, input_dim, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n",
    "\n",
    "class BiLSTMTagger_Pretrained(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, weights_matrix, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding, input_dim, embedding_dim = create_emb_layer(weights_matrix, pad_idx, False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger_Pretrained(du_weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 1.540 | Train Acc: 56.38%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 78.97%\n",
      "Epoch: 02 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.330 | Train Acc: 90.82%\n",
      "\t Val. Loss: 0.306 |  Val. Acc: 91.18%\n",
      "Epoch: 03 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.135 | Train Acc: 96.36%\n",
      "\t Val. Loss: 0.255 |  Val. Acc: 91.90%\n",
      "Epoch: 04 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.086 | Train Acc: 97.47%\n",
      "\t Val. Loss: 0.251 |  Val. Acc: 92.04%\n",
      "Epoch: 05 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.063 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 92.14%\n",
      "Epoch: 06 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.63%\n",
      "\t Val. Loss: 0.253 |  Val. Acc: 92.01%\n",
      "Epoch: 07 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.036 | Train Acc: 99.02%\n",
      "\t Val. Loss: 0.268 |  Val. Acc: 91.90%\n",
      "Epoch: 08 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.32%\n",
      "\t Val. Loss: 0.267 |  Val. Acc: 92.02%\n",
      "Epoch: 09 | Epoch Time: 1m 4s\n",
      "\tTrain Loss: 0.020 | Train Acc: 99.54%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 92.11%\n",
      "Epoch: 10 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.70%\n",
      "\t Val. Loss: 0.285 |  Val. Acc: 91.99%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.011 |  Test Acc: 99.82%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0132, -0.1399,  0.0156,  ...,  0.1987, -0.1969, -0.2001],\n",
      "        ...,\n",
      "        [-0.0255,  0.0027, -0.0249,  ..., -0.0357, -0.0044,  0.0078],\n",
      "        [ 0.0369, -0.0191, -0.0288,  ...,  0.0303, -0.0024,  0.0262],\n",
      "        [ 0.0430, -0.0317, -0.0201,  ...,  0.0229, -0.0307, -0.0319]],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.0518, -0.0665,  0.0635,  ..., -0.0891, -0.0785,  0.0975],\n",
      "        [ 0.1349,  0.0752, -0.0074,  ...,  0.0438, -0.0572,  0.0259],\n",
      "        [ 0.1003, -0.0514,  0.0247,  ..., -0.0460,  0.0027, -0.0019],\n",
      "        ...,\n",
      "        [-0.0571, -0.0767,  0.0433,  ...,  0.1875, -0.0237, -0.0743],\n",
      "        [ 0.0813, -0.0645,  0.0178,  ..., -0.0551, -0.0817, -0.0951],\n",
      "        [ 0.0786, -0.1381,  0.0993,  ...,  0.0071,  0.0202, -0.1401]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.0240, -0.1434, -0.0220,  ..., -0.1434, -0.1997,  0.0578],\n",
      "        [-0.1269, -0.0944,  0.0536,  ..., -0.0489, -0.0563,  0.0482],\n",
      "        [-0.0144, -0.1543, -0.0078,  ..., -0.1431, -0.0538,  0.1316],\n",
      "        ...,\n",
      "        [ 0.0054, -0.0198,  0.0435,  ..., -0.0799,  0.1718,  0.0816],\n",
      "        [-0.1119, -0.0891,  0.1025,  ..., -0.1094, -0.0372,  0.0341],\n",
      "        [-0.1530, -0.0392,  0.0988,  ..., -0.0997, -0.0427,  0.0103]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([ 1.3838e-01,  3.1448e-02,  3.8116e-02, -1.4066e-02,  1.2838e-01,\n",
      "         2.9692e-02,  1.1615e-01, -2.0439e-03,  1.1689e-01,  1.5893e-01,\n",
      "         3.2175e-02,  1.8293e-01,  1.9279e-01,  2.9699e-02,  4.6634e-02,\n",
      "         1.2646e-01,  1.1869e-02,  1.7951e-01,  1.3799e-01,  6.0080e-02,\n",
      "         7.1783e-02,  1.1718e-01,  1.0961e-01,  5.2751e-02,  1.6879e-01,\n",
      "         1.4057e-01,  8.9785e-02,  1.9538e-01,  1.4931e-01,  5.6005e-02,\n",
      "         3.4354e-02,  1.6812e-01,  3.3319e-02,  1.9728e-01,  1.5463e-01,\n",
      "         1.7867e-01,  1.6561e-01,  8.5952e-02,  5.7436e-02,  6.6009e-02,\n",
      "         1.5112e-01,  1.8017e-01,  6.5485e-02,  1.4023e-01, -2.1773e-03,\n",
      "         1.4054e-01,  1.4875e-01,  5.8591e-02,  1.3387e-01,  9.3811e-02,\n",
      "         1.3368e-01,  6.9193e-02,  1.3163e-01,  1.0090e-01,  9.3382e-02,\n",
      "         7.3170e-03,  1.6570e-01,  4.1574e-02,  3.3703e-02,  9.9642e-02,\n",
      "         1.0819e-01,  1.5974e-01,  3.1957e-02, -2.0839e-02,  2.3959e-02,\n",
      "         1.2172e-01,  2.3004e-02,  6.8767e-02,  7.8231e-02,  5.6619e-02,\n",
      "         3.8548e-03,  5.1987e-02,  2.2025e-01,  1.7914e-02,  1.2660e-01,\n",
      "         6.1841e-02,  1.8686e-01,  1.9163e-01,  1.1729e-01,  3.1533e-02,\n",
      "         1.6340e-01,  1.6946e-01,  1.0208e-02,  1.7109e-01,  8.5787e-02,\n",
      "         1.2654e-01,  5.0446e-02,  9.6674e-02,  1.3703e-01,  1.5428e-01,\n",
      "         1.3745e-01,  1.8029e-01,  1.5974e-01,  8.5266e-02,  1.8298e-01,\n",
      "         8.2800e-02,  1.6926e-01,  8.2602e-02,  6.5354e-02,  3.7350e-02,\n",
      "         1.7788e-01,  9.4603e-02,  5.3076e-03,  1.6968e-01,  8.4980e-02,\n",
      "         1.5207e-01,  2.9545e-02,  2.9710e-02,  8.2500e-02,  1.3943e-01,\n",
      "         7.4029e-02,  1.0596e-01,  6.1728e-02,  1.5930e-01,  1.3428e-01,\n",
      "         1.1070e-01,  2.2271e-01,  1.7249e-01,  6.6227e-02,  1.0633e-01,\n",
      "         6.6616e-02,  1.5330e-01,  5.1133e-02, -2.4850e-03,  7.5975e-02,\n",
      "         5.0572e-03,  9.3739e-02,  1.8111e-01, -7.6939e-02,  6.0925e-02,\n",
      "        -2.8649e-02,  8.6675e-02,  1.1273e-01,  2.6114e-02, -4.2501e-02,\n",
      "         3.7454e-02, -5.0869e-02,  8.7283e-02,  3.4102e-02, -8.9031e-02,\n",
      "         1.1946e-02, -8.6498e-02, -5.7004e-02, -2.9519e-03, -1.8799e-02,\n",
      "         6.4297e-02, -8.8672e-02,  5.1344e-02,  1.0220e-01, -1.0066e-02,\n",
      "         9.7343e-02, -2.2638e-02,  6.1642e-02,  1.7355e-03,  2.9678e-02,\n",
      "         1.3367e-02,  1.0267e-01,  3.3774e-02, -9.7619e-02,  5.2366e-02,\n",
      "        -3.6709e-02, -4.2939e-02, -4.6867e-02, -3.3388e-02,  3.7705e-02,\n",
      "         6.0090e-02, -9.0594e-02,  3.0503e-02,  8.2878e-02, -2.9280e-02,\n",
      "         3.3795e-02,  9.0582e-02,  4.8942e-02,  1.8589e-02, -1.8052e-02,\n",
      "        -2.0032e-02,  7.7628e-02,  5.7780e-02,  2.6823e-02, -5.9408e-02,\n",
      "         2.1746e-02, -8.5947e-02,  3.6447e-02,  1.0382e-01, -5.8127e-02,\n",
      "         8.1631e-02, -8.3759e-02,  1.8300e-02, -6.7724e-02,  9.7294e-03,\n",
      "         9.3453e-02,  3.8901e-02,  7.1897e-02,  2.7531e-02,  5.8589e-02,\n",
      "        -4.2845e-02,  5.0396e-02, -1.1546e-03,  6.4429e-03, -3.0136e-02,\n",
      "        -9.1394e-02,  4.5934e-02,  3.5075e-02, -2.1305e-02,  3.1043e-02,\n",
      "        -2.8792e-02, -2.9390e-02,  5.9338e-02, -1.1874e-01, -1.8730e-02,\n",
      "         1.5685e-02, -5.1499e-02, -5.7552e-02,  1.4814e-01, -5.0963e-03,\n",
      "         5.4896e-02,  8.9455e-02, -4.7549e-02, -9.0296e-02, -7.7179e-03,\n",
      "         2.1616e-02,  1.5476e-02, -7.0230e-02, -6.3029e-02,  5.3410e-02,\n",
      "         3.3406e-02, -3.7589e-02, -6.3389e-03, -9.7178e-02, -6.6330e-04,\n",
      "         4.0051e-02,  4.7726e-03, -3.1062e-02,  1.3346e-01,  1.2777e-02,\n",
      "         2.9071e-03, -8.2005e-02, -6.2479e-02,  3.7213e-02,  1.2879e-01,\n",
      "        -7.5189e-02, -9.3062e-02,  9.3319e-02, -9.6005e-03, -5.3863e-03,\n",
      "        -8.2961e-02, -3.3476e-02, -2.5912e-02, -1.9337e-02,  3.3230e-02,\n",
      "         9.4319e-02,  5.2445e-02, -4.8191e-02, -1.0271e-03, -4.4141e-02,\n",
      "        -7.2662e-03,  4.9914e-02, -1.3035e-01,  2.6736e-02, -2.0704e-03,\n",
      "        -1.6143e-02,  7.6378e-02, -9.7929e-03, -8.4121e-02,  1.1082e-01,\n",
      "         3.9372e-02, -1.2345e-01, -2.1081e-02,  9.3099e-02,  1.0418e-01,\n",
      "         1.6297e-02, -7.4604e-02,  7.5051e-02, -4.4178e-02,  2.7083e-03,\n",
      "         1.1396e-01, -3.0052e-02,  1.0680e-03,  3.6312e-02,  8.3754e-02,\n",
      "        -7.5047e-02,  2.4571e-02,  4.2990e-02, -3.1355e-03,  9.5108e-03,\n",
      "         8.0468e-02, -4.2086e-02, -1.4153e-02, -6.1882e-02,  1.9655e-02,\n",
      "         2.3810e-02, -1.1644e-01, -3.6212e-03, -9.5566e-02, -2.8848e-02,\n",
      "        -2.5860e-02,  9.6664e-02,  8.6927e-03, -2.1182e-02,  8.7773e-02,\n",
      "        -1.3756e-02,  6.7965e-02, -4.7941e-02, -6.4641e-02,  5.5453e-02,\n",
      "        -1.3370e-01, -9.1825e-02, -7.7094e-02, -2.4797e-02,  1.9482e-02,\n",
      "         8.6284e-02, -4.8385e-02, -3.8173e-02, -8.0010e-02,  1.4764e-02,\n",
      "         3.0401e-02,  7.5471e-05,  6.5266e-02,  1.3832e-01, -9.3034e-02,\n",
      "        -5.5385e-02,  4.5065e-02, -7.3704e-03, -7.8253e-02,  1.3046e-01,\n",
      "        -8.2407e-02, -8.0670e-02,  7.5342e-02,  6.1784e-02,  8.0919e-02,\n",
      "        -8.4361e-02,  1.4375e-01, -8.3911e-03,  4.0572e-02,  5.2168e-03,\n",
      "         4.3026e-02, -9.1275e-03,  6.7160e-02, -9.3446e-02, -4.9575e-02,\n",
      "         7.9951e-02, -1.0408e-01,  2.3838e-03,  2.2764e-02,  9.1365e-02,\n",
      "        -1.5015e-01,  4.1390e-02,  3.5096e-02,  4.3886e-03, -1.6789e-02,\n",
      "         6.1825e-02,  4.3952e-02,  1.1927e-01,  1.9002e-02, -5.4158e-02,\n",
      "        -3.4973e-02,  6.5489e-02, -1.3259e-02, -1.2086e-02, -7.5327e-02,\n",
      "        -2.4069e-03,  9.9686e-02, -5.5665e-03, -9.5256e-02, -4.9545e-04,\n",
      "         3.6698e-02,  1.0414e-01, -1.6525e-01,  5.9404e-02, -7.0051e-02,\n",
      "         5.5742e-02,  2.2123e-02,  6.5293e-02,  2.4402e-02, -3.3817e-02,\n",
      "        -6.5982e-02,  5.3915e-02,  1.1389e-01, -1.1748e-01,  1.2148e-01,\n",
      "         6.0907e-02, -6.5290e-02, -5.0342e-02, -1.4990e-02,  1.1331e-01,\n",
      "         1.3734e-01,  1.8351e-01,  1.1276e-01,  1.0671e-01,  1.1450e-02,\n",
      "         1.8146e-01,  4.7718e-02,  1.6333e-01,  7.2583e-02,  8.4391e-02,\n",
      "         1.2932e-01,  2.8170e-02,  4.1727e-02,  2.9652e-02,  7.2946e-02,\n",
      "        -1.2831e-02,  1.2129e-01,  5.6011e-02,  8.7846e-02, -1.3895e-02,\n",
      "         1.8873e-01, -2.8397e-02, -2.7427e-02,  7.7426e-02,  1.6351e-01,\n",
      "         1.6674e-01,  1.0706e-01,  1.2764e-01,  1.2220e-01,  1.4191e-01,\n",
      "         1.1537e-01,  1.0490e-01,  6.3130e-02,  8.0748e-02,  1.2758e-01,\n",
      "         8.8998e-02,  4.4777e-02,  1.0931e-01,  1.7558e-01,  5.1752e-02,\n",
      "         1.1709e-01,  7.6419e-02,  1.4362e-01,  7.5136e-02,  2.5174e-02,\n",
      "         1.5947e-01,  1.4604e-02,  1.2948e-01,  1.7520e-01,  6.1583e-02,\n",
      "         1.7798e-01,  1.3503e-02,  1.8214e-01,  1.0928e-01,  7.3099e-02,\n",
      "         6.3271e-02,  4.9446e-02,  3.0343e-02,  1.7721e-01,  4.6536e-02,\n",
      "         4.5436e-02,  7.3740e-02,  1.3816e-02,  9.6698e-02,  1.6089e-01,\n",
      "         1.4979e-01,  4.5869e-02,  1.1391e-01,  1.2932e-01,  1.4106e-01,\n",
      "         3.4636e-02,  1.3304e-01,  2.4964e-02,  5.9042e-02,  1.4991e-01,\n",
      "         1.9084e-01,  1.7551e-01,  1.4715e-01,  1.3634e-01,  1.6893e-01,\n",
      "         3.4514e-02,  1.0098e-01,  1.4429e-01,  6.5863e-02,  8.3448e-03,\n",
      "         1.1166e-01,  3.8876e-02,  1.1004e-01,  1.1505e-01,  1.3243e-01,\n",
      "         1.4859e-01,  4.2886e-02,  1.7448e-01,  1.5873e-01,  3.7199e-02,\n",
      "         1.2064e-01,  1.9626e-01,  1.0528e-01,  8.7936e-02,  3.8317e-02,\n",
      "         1.1475e-01, -5.2348e-04,  1.0949e-01,  2.0604e-01,  1.3024e-01,\n",
      "         6.7835e-02,  3.2185e-02,  8.7065e-02,  5.0877e-02,  1.0464e-01,\n",
      "         1.4805e-01,  1.9382e-02,  4.3246e-02, -1.2013e-02,  9.6322e-02,\n",
      "         6.0269e-02,  2.8000e-02,  8.1794e-02,  1.1154e-01,  1.6255e-01,\n",
      "         1.6165e-01,  3.7400e-02, -2.8339e-02,  9.2859e-02, -4.3681e-03,\n",
      "         2.0636e-01,  1.5713e-03], requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 1.7165e-01,  2.7584e-02,  1.8574e-01,  1.4787e-01, -5.8670e-03,\n",
      "         1.2044e-01,  9.9581e-02,  1.2846e-01,  5.7994e-02,  1.5738e-01,\n",
      "         1.5126e-01,  1.2086e-01,  8.6473e-02,  1.2232e-01,  1.6647e-01,\n",
      "         1.5588e-01,  1.1377e-01,  6.2486e-02,  2.0175e-02,  1.3163e-01,\n",
      "        -2.6378e-02,  7.9462e-02,  1.3611e-01,  1.3786e-01,  1.8140e-01,\n",
      "         1.2831e-01,  9.3309e-02,  1.6378e-01,  5.5905e-02,  2.0791e-02,\n",
      "         5.8791e-02,  6.9544e-02,  3.3866e-02,  2.1129e-01,  1.6759e-01,\n",
      "         1.0047e-01,  1.1265e-01,  4.0626e-02,  8.1925e-02,  3.8083e-02,\n",
      "         7.3552e-02,  1.2553e-01,  6.7717e-02,  3.0603e-02,  1.3819e-01,\n",
      "         8.2573e-02,  4.3749e-02,  1.0371e-01,  1.7454e-01,  2.2753e-01,\n",
      "         1.0322e-01,  1.1440e-01, -2.2955e-03,  9.6926e-02,  5.9479e-02,\n",
      "         6.4301e-02,  9.3508e-02,  6.2917e-02,  1.0878e-01,  1.7982e-01,\n",
      "         6.8514e-02,  1.0435e-01,  1.3459e-01,  1.1738e-01,  1.7280e-01,\n",
      "        -6.6430e-03,  3.6203e-02, -2.9668e-02,  4.5445e-02,  1.6562e-01,\n",
      "         8.0989e-04,  7.9133e-02,  1.0519e-01,  1.9606e-02,  1.1620e-01,\n",
      "         1.2259e-01,  8.1496e-02,  1.6416e-01,  8.2969e-02,  1.5121e-01,\n",
      "         4.4481e-02,  9.9524e-02,  1.0074e-01,  5.5761e-02,  6.2150e-02,\n",
      "         1.2192e-01,  1.2261e-01,  6.8011e-03,  6.1961e-02,  1.6579e-01,\n",
      "         8.2679e-02,  2.2255e-01,  1.6533e-01,  4.9273e-02,  7.4984e-02,\n",
      "         1.2214e-01,  1.7393e-01,  9.6137e-02,  1.8449e-01,  6.5784e-02,\n",
      "         8.7203e-02,  1.9331e-02,  9.4887e-02,  1.4180e-01,  1.0788e-01,\n",
      "         1.5948e-01,  4.0733e-02,  4.7896e-02,  1.4864e-01,  1.4399e-01,\n",
      "         1.1977e-01,  2.0363e-01,  1.2611e-01,  2.0717e-01, -1.6898e-02,\n",
      "         1.0777e-01,  1.6657e-01,  1.6428e-01,  1.8418e-01,  1.4600e-01,\n",
      "         1.1054e-01,  8.3041e-02,  2.0750e-02,  1.0052e-01,  2.7813e-02,\n",
      "         3.8099e-02,  1.1619e-01,  6.4360e-02,  6.2573e-02,  6.8690e-02,\n",
      "         7.2131e-02,  8.4861e-02,  4.3509e-02,  5.2144e-02,  1.1721e-02,\n",
      "         5.9250e-02,  7.6717e-03,  5.3514e-02,  1.1157e-01,  7.2349e-03,\n",
      "        -1.0647e-02, -6.0505e-02, -3.4168e-02,  1.0719e-01,  8.3653e-02,\n",
      "         3.9100e-03, -4.8432e-02, -2.4586e-02, -4.1030e-02, -3.1637e-02,\n",
      "         6.1388e-02,  2.7803e-02, -3.6407e-02,  4.1512e-02,  3.7780e-02,\n",
      "         5.2522e-02, -5.6437e-02, -1.4629e-02, -1.2608e-01,  7.8799e-02,\n",
      "         1.6066e-02,  5.3673e-03, -1.1077e-01,  5.1929e-02,  1.1748e-02,\n",
      "         6.0772e-02,  7.3802e-02, -4.2753e-03,  1.2099e-01,  3.2879e-02,\n",
      "         1.6675e-02,  3.6593e-02, -3.5451e-02,  8.8423e-02, -5.9184e-02,\n",
      "         6.1117e-02,  6.9021e-02,  3.6085e-02, -5.0810e-02, -5.1697e-02,\n",
      "        -3.5357e-02, -2.3999e-02, -5.2562e-02,  4.8670e-02,  8.6201e-02,\n",
      "        -2.1848e-02,  6.9450e-02, -1.3347e-01,  7.6808e-02,  1.6591e-03,\n",
      "         7.8748e-02, -7.1378e-02, -1.3940e-02, -1.6875e-02, -8.1513e-03,\n",
      "         1.1591e-02, -2.4213e-02, -3.6139e-02,  6.8403e-02, -5.6473e-02,\n",
      "        -1.4393e-01,  7.5478e-02,  1.9837e-02,  9.2083e-02, -1.3386e-01,\n",
      "         3.7139e-03,  1.2553e-02,  8.0427e-02, -3.2202e-02, -3.7000e-02,\n",
      "         2.3650e-02, -4.1317e-03, -4.9663e-02,  8.8838e-02, -5.0638e-02,\n",
      "         4.5582e-03,  1.7475e-02,  7.2581e-02, -3.9277e-02, -3.8905e-02,\n",
      "         4.2042e-02, -4.8110e-02, -3.4157e-02, -2.6925e-02, -6.2910e-02,\n",
      "        -6.5137e-02,  3.4264e-02,  3.2995e-03, -5.1742e-02, -9.9206e-03,\n",
      "        -3.5122e-02, -8.7676e-02,  9.2728e-02,  1.8438e-02, -2.1304e-03,\n",
      "        -2.6671e-02, -2.6530e-02,  7.0109e-02, -2.8264e-02,  1.1373e-01,\n",
      "        -8.7031e-03, -1.5664e-02,  1.6570e-02, -8.3412e-02, -9.0085e-02,\n",
      "         8.3191e-02,  7.9493e-02, -1.8396e-02, -8.1280e-02,  7.6147e-02,\n",
      "         1.7632e-02, -3.7496e-02,  4.7633e-02,  6.7860e-02, -1.3984e-02,\n",
      "        -4.4919e-02, -1.0143e-01, -6.4965e-02, -5.6344e-02, -1.3322e-01,\n",
      "         4.1929e-02,  9.5988e-02, -4.0935e-02, -1.2425e-01,  1.5806e-02,\n",
      "         4.4786e-03, -1.2580e-01, -1.3012e-02,  5.2448e-02,  1.0488e-02,\n",
      "         6.2285e-02, -1.4286e-02,  7.3315e-02, -1.1302e-01, -7.1798e-02,\n",
      "         1.3713e-01,  1.1264e-01,  3.5541e-02, -1.1042e-01,  1.2347e-01,\n",
      "        -5.7949e-02,  1.3481e-01,  8.7245e-02,  1.9851e-02, -6.6476e-03,\n",
      "         5.4155e-02, -8.6296e-02,  9.5374e-03, -1.2134e-01,  6.2016e-02,\n",
      "        -1.5934e-02, -8.4762e-02,  1.5330e-02,  5.3970e-03, -6.1369e-05,\n",
      "        -6.7753e-02,  4.4131e-02, -1.1908e-01,  3.1890e-02, -2.7657e-02,\n",
      "         7.3426e-02,  1.1979e-01, -1.8555e-02,  4.6919e-03, -1.1950e-02,\n",
      "        -9.4200e-02, -3.1031e-02, -7.1931e-02,  1.0118e-01,  2.9708e-03,\n",
      "         2.4344e-02, -7.5791e-02, -6.7017e-02, -1.1987e-01, -1.6048e-03,\n",
      "         5.5124e-02, -7.1550e-03,  4.5502e-02,  1.3623e-01,  4.2987e-02,\n",
      "        -8.2628e-02, -5.7576e-03, -7.4365e-02, -1.1309e-01,  1.3602e-01,\n",
      "        -9.2416e-02, -9.2704e-02,  3.2881e-02,  2.1032e-02, -3.5324e-02,\n",
      "        -6.3201e-02,  8.8438e-02,  8.5759e-03, -2.5156e-02,  5.0273e-02,\n",
      "         1.3471e-02,  8.0664e-02,  1.4089e-02,  2.4178e-02,  3.1002e-02,\n",
      "        -2.6876e-02, -1.4051e-01, -6.3293e-02, -1.3145e-01,  9.5934e-02,\n",
      "        -1.5268e-01,  9.3323e-02, -1.3038e-01,  2.3347e-02,  6.8091e-02,\n",
      "         1.1558e-01,  4.1145e-02,  7.6594e-03, -3.3113e-02, -2.8162e-02,\n",
      "         6.7974e-02, -5.3885e-02, -1.5634e-02, -1.1496e-01, -1.3795e-01,\n",
      "         2.9840e-02,  7.7600e-02, -1.0124e-01, -9.5885e-02,  1.0245e-01,\n",
      "        -1.0751e-01,  3.7394e-02, -8.2513e-02, -1.0642e-01, -2.6534e-02,\n",
      "        -9.6107e-02,  6.3390e-03,  5.8448e-02, -9.1369e-03, -3.6009e-02,\n",
      "        -4.3173e-02,  7.8891e-02,  5.4824e-02, -1.2421e-01,  9.1394e-02,\n",
      "        -1.3056e-02, -1.7860e-02, -7.2308e-02,  1.0889e-01,  1.7356e-01,\n",
      "         1.8172e-02,  1.8475e-01,  1.0271e-01,  5.6616e-02,  4.3455e-02,\n",
      "         1.1082e-01,  1.0839e-01,  1.6997e-01,  9.7618e-02,  7.8750e-02,\n",
      "         5.0472e-02,  1.6599e-01,  6.7768e-02,  3.5281e-02,  1.2652e-01,\n",
      "         1.7772e-02,  1.5713e-01,  1.6240e-01,  1.0181e-02, -1.7778e-02,\n",
      "         1.5699e-01,  1.0468e-01,  8.2994e-04,  4.3671e-02,  1.6101e-01,\n",
      "         9.0229e-02,  1.9369e-01,  1.0833e-01,  1.2892e-01,  1.7022e-01,\n",
      "         1.0593e-01,  1.1728e-01,  1.3006e-01,  9.7635e-02,  1.2071e-01,\n",
      "         1.4811e-01,  5.5407e-02,  1.7720e-01,  1.7571e-01,  6.4244e-02,\n",
      "         7.9238e-02,  2.0321e-02,  1.8001e-01,  3.1187e-02, -2.3876e-02,\n",
      "         1.6072e-01,  5.2879e-02,  1.2058e-01,  2.2301e-01,  5.3685e-02,\n",
      "         1.5623e-01,  1.1860e-01,  1.2319e-01,  2.9137e-02,  1.9574e-02,\n",
      "         1.4549e-01,  9.4312e-03,  5.5461e-02,  1.0180e-01,  1.5329e-01,\n",
      "         3.4324e-02,  2.9093e-02, -2.3478e-03,  1.5472e-01,  1.2860e-01,\n",
      "         4.5488e-02,  1.0158e-01,  7.6110e-02,  1.9079e-01,  9.5145e-02,\n",
      "         1.8196e-01,  1.0518e-01,  7.9125e-02,  7.9312e-02,  1.7844e-02,\n",
      "         1.4933e-01,  1.1629e-02,  1.2985e-02,  1.0718e-01,  1.0966e-01,\n",
      "         1.6893e-01,  7.0501e-02,  4.9643e-02,  2.6146e-02,  1.4796e-01,\n",
      "         1.5604e-01,  6.4687e-02,  7.5443e-02,  1.0563e-01,  1.0663e-01,\n",
      "         1.1037e-01,  1.4356e-01,  7.5189e-02,  9.0434e-02,  1.7194e-01,\n",
      "         1.7427e-01,  1.2616e-01,  1.3329e-01,  1.2628e-01,  8.0525e-02,\n",
      "         1.0865e-01,  1.2507e-01,  2.5040e-01,  1.4528e-01,  8.3461e-02,\n",
      "         2.8548e-02,  1.0603e-01,  2.2552e-02,  1.0178e-01,  6.6959e-02,\n",
      "         1.8367e-01,  4.6446e-02,  1.1591e-01, -7.9452e-03,  9.8245e-02,\n",
      "         1.3530e-01,  3.0611e-02,  1.5549e-01,  8.5723e-02,  9.1491e-02,\n",
      "         4.6199e-02,  6.0334e-02,  8.4008e-02,  8.8570e-02,  4.5680e-03,\n",
      "         9.4486e-02,  1.3795e-02], requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0080, -0.0038,  0.0313,  ..., -0.0268, -0.0284,  0.0801],\n",
      "        [ 0.1085, -0.0860, -0.0015,  ..., -0.0986, -0.0078, -0.0372],\n",
      "        [ 0.1329, -0.1039,  0.0343,  ..., -0.0742, -0.1525,  0.0767],\n",
      "        ...,\n",
      "        [-0.0112,  0.0408, -0.0235,  ..., -0.0454, -0.0482, -0.0344],\n",
      "        [-0.0045, -0.1829, -0.0311,  ..., -0.1123, -0.0569, -0.1662],\n",
      "        [ 0.1060, -0.1320,  0.0210,  ...,  0.0347, -0.1238, -0.1755]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0048,  0.0978, -0.0177,  ...,  0.0518,  0.1261,  0.1158],\n",
      "        [ 0.1031,  0.1236, -0.0768,  ..., -0.0838,  0.0653,  0.0781],\n",
      "        [ 0.1388,  0.0519, -0.0966,  ..., -0.1004,  0.0704,  0.1320],\n",
      "        ...,\n",
      "        [ 0.1092,  0.1555, -0.1080,  ..., -0.1196,  0.0509,  0.0104],\n",
      "        [-0.0568,  0.2191, -0.0049,  ..., -0.1159,  0.0436,  0.0551],\n",
      "        [ 0.0373,  0.0647, -0.0744,  ..., -0.1151, -0.0186,  0.0542]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([-2.0317e-02,  1.5910e-01,  1.1520e-01,  6.6202e-02, -1.5686e-03,\n",
      "         1.4857e-01,  1.4994e-01,  3.4740e-02,  1.3614e-01,  1.2875e-01,\n",
      "         8.0341e-02,  1.0989e-01,  8.1624e-02,  8.5635e-03,  9.2681e-02,\n",
      "        -1.3596e-02,  8.4787e-02,  8.3801e-02, -6.2352e-03,  1.0326e-01,\n",
      "         2.9332e-02,  1.1764e-01,  9.1648e-02,  3.2516e-02,  1.8760e-02,\n",
      "         6.1750e-02, -9.6498e-03,  5.9045e-03, -2.5748e-02,  6.6940e-02,\n",
      "         8.6752e-02, -3.8210e-03,  5.4139e-04,  1.3439e-01,  7.7479e-02,\n",
      "         9.2634e-02,  3.7665e-02,  1.6718e-02,  8.9387e-02,  5.7339e-02,\n",
      "         7.9753e-02,  1.2698e-01,  1.1411e-01,  1.1147e-01,  4.9218e-02,\n",
      "        -2.9183e-02,  1.5173e-01,  2.9183e-02,  1.0281e-02,  1.0921e-01,\n",
      "        -2.9312e-02,  3.2923e-02,  8.0289e-02,  2.4009e-02,  8.0354e-02,\n",
      "         1.0614e-01,  1.3274e-01,  6.7251e-02,  3.8642e-02, -1.0454e-02,\n",
      "         5.7916e-02,  6.7601e-03,  1.1993e-02, -1.4095e-02,  8.5754e-02,\n",
      "         6.5577e-02,  9.8426e-02,  1.1839e-01,  1.2447e-01,  2.5485e-02,\n",
      "         1.2980e-01,  6.5747e-02,  1.2251e-01,  2.9426e-03,  4.8782e-02,\n",
      "         1.5111e-01,  7.5244e-03, -4.5711e-02,  1.2820e-01,  4.8407e-02,\n",
      "         8.1956e-02,  5.0124e-02,  7.6458e-02,  4.5687e-02,  1.5817e-01,\n",
      "         1.4260e-01,  1.5666e-01,  1.3995e-01,  5.1668e-02,  9.0530e-02,\n",
      "         1.4042e-01,  1.3509e-01,  9.1608e-02,  1.2409e-01,  6.6926e-02,\n",
      "         1.7182e-01,  1.2634e-01,  1.4687e-01,  1.0403e-01,  3.7388e-02,\n",
      "         4.8070e-02,  4.0161e-02,  6.7796e-03,  1.1356e-02,  4.1788e-04,\n",
      "         4.3666e-02,  2.8026e-03,  1.0793e-01,  4.9193e-02,  4.0748e-02,\n",
      "         4.5565e-02,  1.1153e-01,  1.7090e-01,  8.4809e-02,  3.7591e-02,\n",
      "         3.3859e-02, -1.2690e-02,  1.0592e-02,  9.8588e-02,  9.3833e-02,\n",
      "         7.2505e-02,  7.2342e-02,  1.8311e-01,  1.9333e-02,  3.4305e-03,\n",
      "         8.3574e-02,  1.2043e-02,  1.0847e-01,  4.5601e-02, -6.2421e-02,\n",
      "        -9.2213e-02,  4.1934e-02, -4.6509e-02, -4.9258e-02, -2.0972e-02,\n",
      "         1.5216e-02, -6.0860e-02,  3.1766e-02,  6.5464e-02, -3.6016e-02,\n",
      "         2.9302e-02, -6.3426e-02,  5.4966e-02,  6.6857e-02, -1.0966e-01,\n",
      "        -2.4280e-02, -7.5888e-03, -4.8536e-02,  3.3795e-02,  3.5061e-05,\n",
      "        -6.6712e-02,  5.9310e-02,  1.5283e-02,  2.8838e-02, -2.6835e-03,\n",
      "         8.9515e-02, -1.0884e-02,  1.1106e-02,  1.2438e-01,  1.1975e-01,\n",
      "         3.0417e-02, -5.0927e-03,  6.0343e-02, -3.4723e-02, -9.9873e-02,\n",
      "        -4.2611e-02,  3.1083e-02,  3.8600e-03, -4.6992e-02, -2.8419e-02,\n",
      "        -4.8361e-02, -4.5812e-02,  5.5860e-02, -4.7687e-02, -2.0515e-02,\n",
      "        -7.2732e-02, -3.2192e-02, -1.1468e-01, -1.0534e-02,  3.7033e-02,\n",
      "        -7.9489e-02, -3.9720e-02, -1.9037e-02,  9.0508e-02, -6.0961e-02,\n",
      "         7.1916e-02, -8.5452e-02, -5.6724e-02,  6.9564e-03, -1.5810e-02,\n",
      "         8.0944e-02,  2.1437e-02, -5.9383e-02, -1.0118e-01,  6.5731e-02,\n",
      "        -2.6835e-02,  1.1736e-02, -8.1887e-02,  4.1397e-03, -7.8077e-02,\n",
      "        -9.5396e-02, -5.8478e-02, -1.0593e-02, -2.9813e-02,  8.4641e-02,\n",
      "         9.1860e-02,  5.3963e-02, -3.7603e-02,  4.7205e-02, -3.6309e-02,\n",
      "         4.8695e-02,  2.3435e-02,  8.2609e-03,  3.5881e-02, -1.0557e-01,\n",
      "         6.0036e-02,  9.2569e-03, -8.2009e-02,  1.0288e-01, -4.6426e-02,\n",
      "         3.9556e-02,  2.1465e-02, -5.9565e-02, -1.7709e-02, -1.2065e-02,\n",
      "        -5.8019e-02,  7.3718e-02,  9.1381e-02,  4.5049e-02, -5.2587e-02,\n",
      "         7.3218e-02,  6.5250e-02,  3.8419e-02,  1.4252e-02,  6.2072e-02,\n",
      "        -3.4779e-02,  1.9325e-02,  8.3247e-02,  4.8421e-02,  1.6711e-02,\n",
      "        -6.7779e-02, -1.6461e-02,  2.4391e-02,  3.7359e-02,  5.8322e-02,\n",
      "        -4.2432e-02, -2.3212e-02, -2.2757e-02,  3.3845e-02,  6.7811e-02,\n",
      "        -8.3904e-02, -7.6307e-02,  3.3118e-02, -6.6900e-03, -2.5764e-02,\n",
      "         2.6105e-02,  7.9377e-02, -4.6492e-02, -7.1561e-02,  5.6070e-02,\n",
      "         6.0712e-02, -1.3788e-02,  4.0634e-02, -1.0410e-01, -6.2666e-04,\n",
      "        -7.9915e-02,  4.7880e-02, -1.0280e-01,  1.0865e-01, -6.6093e-03,\n",
      "        -6.0987e-02,  1.0374e-01,  1.0041e-02,  2.7400e-02,  9.9580e-02,\n",
      "        -9.0344e-02,  1.2517e-01, -1.0084e-01,  7.8548e-02, -7.6193e-02,\n",
      "        -1.9136e-02, -2.1692e-02, -1.1280e-01, -7.4711e-02,  2.8889e-03,\n",
      "         6.4538e-02,  8.9159e-02, -8.9155e-02,  7.1828e-02,  4.1606e-02,\n",
      "         6.3135e-02,  6.6661e-02,  4.1351e-02, -1.2285e-02,  5.5064e-02,\n",
      "         4.8929e-02,  6.0817e-02,  4.3551e-02, -8.1694e-02, -1.0120e-01,\n",
      "        -1.0480e-01, -8.9425e-02,  6.9409e-02, -4.9076e-02, -5.9191e-02,\n",
      "        -5.4134e-02, -8.5516e-02, -4.8406e-03, -7.2130e-02, -7.8405e-02,\n",
      "         1.3254e-03, -3.5027e-02, -6.7975e-02, -9.5751e-02, -1.3649e-02,\n",
      "        -5.6159e-02, -2.5796e-02,  6.3950e-02,  8.3489e-02, -2.8714e-02,\n",
      "         6.7537e-02,  5.2741e-02, -2.3452e-03,  4.0005e-02,  9.8135e-02,\n",
      "        -2.6322e-02,  6.4266e-02, -3.9889e-03, -8.8289e-02,  1.0076e-01,\n",
      "        -8.3576e-03,  1.1479e-01,  5.3749e-02,  1.5877e-02,  9.2092e-02,\n",
      "        -7.3111e-02,  1.8634e-03,  8.5370e-02, -4.0452e-02, -7.6661e-03,\n",
      "         6.3558e-02,  7.6033e-02,  1.0651e-02,  3.7647e-03, -5.4884e-02,\n",
      "        -6.0991e-02, -1.2812e-02,  3.8346e-03,  6.1563e-03,  3.8674e-03,\n",
      "        -2.7451e-02,  3.6825e-02, -7.8792e-03,  1.1083e-01,  7.9400e-02,\n",
      "        -7.3930e-02,  8.6956e-03,  1.0041e-01, -1.0597e-01,  1.8209e-02,\n",
      "        -3.0380e-02, -6.6711e-02, -3.3325e-02, -9.7612e-02, -6.4783e-02,\n",
      "        -5.5514e-02, -7.1629e-02,  2.5512e-02,  1.1433e-02, -9.0728e-02,\n",
      "        -8.6634e-03, -3.1578e-02, -7.8465e-02, -2.7362e-02,  2.9239e-02,\n",
      "        -9.3643e-02, -1.0160e-01, -6.8372e-02, -2.9261e-02, -7.3961e-02,\n",
      "        -4.7092e-02, -4.8139e-02, -2.5522e-02,  1.4316e-02, -1.0291e-02,\n",
      "         3.3541e-02,  1.7660e-01,  1.8347e-01,  1.8945e-02,  7.5656e-02,\n",
      "         4.5022e-02,  8.3160e-02,  1.6080e-01,  1.1010e-01,  8.8164e-02,\n",
      "         6.0031e-02,  1.0500e-01,  9.5254e-02, -3.7415e-02,  2.7486e-02,\n",
      "         8.1718e-02,  4.8141e-02,  1.0237e-02,  1.2688e-01,  1.5455e-01,\n",
      "         9.6707e-03,  1.0035e-01,  9.7233e-02,  7.6962e-02,  8.4843e-02,\n",
      "         2.1592e-02, -2.2154e-02,  1.0495e-01, -1.9244e-03, -8.9813e-03,\n",
      "         1.1015e-01,  2.1786e-02,  2.6967e-02,  5.1497e-02, -1.8595e-02,\n",
      "         1.3339e-01,  9.7206e-02,  7.6787e-03, -1.2637e-02,  8.0523e-02,\n",
      "         1.0993e-01,  1.2453e-01, -2.1702e-02,  1.6602e-01, -3.9494e-02,\n",
      "         1.3640e-01, -1.0848e-02,  1.4726e-01,  2.3062e-02,  2.8749e-02,\n",
      "        -4.5774e-03,  4.2431e-02,  1.1581e-01,  2.6451e-02,  4.3773e-02,\n",
      "         5.2380e-02,  1.3186e-02,  9.1482e-03, -5.0667e-04,  1.6888e-01,\n",
      "         1.9314e-02,  4.5033e-02,  6.7419e-02,  7.5281e-02,  4.4243e-02,\n",
      "         1.3146e-01, -5.7955e-03,  7.2780e-02, -6.1948e-03,  9.5111e-02,\n",
      "         8.2913e-02,  8.4640e-02, -1.0202e-02,  6.4367e-02,  8.5772e-02,\n",
      "         5.3433e-02, -4.0945e-03,  5.0946e-02,  1.0488e-02,  1.2151e-01,\n",
      "        -2.1040e-02,  2.2955e-02,  9.7033e-02,  3.0273e-02,  1.1461e-01,\n",
      "         1.0292e-01,  8.4434e-03,  1.1469e-02,  8.3517e-02,  6.3258e-02,\n",
      "         1.4432e-01, -4.6453e-02,  1.7538e-01,  1.2722e-01,  2.9815e-02,\n",
      "         1.1062e-01,  5.0717e-02,  4.2489e-02,  6.5381e-02,  1.2357e-01,\n",
      "        -1.8255e-02,  6.8057e-02, -1.0034e-02,  8.3044e-03,  9.7102e-02,\n",
      "         2.5080e-02,  2.8649e-02,  2.2214e-03,  8.6030e-02,  7.0564e-02,\n",
      "         1.6498e-01,  1.5938e-01,  7.6247e-02,  1.7958e-01,  5.7986e-02,\n",
      "        -3.1290e-02,  4.9066e-02,  1.6151e-01,  3.2247e-02,  1.3319e-01,\n",
      "         2.7083e-03,  1.5819e-01,  1.2985e-01,  8.3356e-02,  8.9989e-02,\n",
      "         3.6754e-02,  8.8621e-02], requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([ 0.0976,  0.1206,  0.0528,  0.0904,  0.1258,  0.0572,  0.0884,  0.0866,\n",
      "         0.0537,  0.0825,  0.1034,  0.0281,  0.0979,  0.0794,  0.0882,  0.0033,\n",
      "         0.0850,  0.1249,  0.0649,  0.0546,  0.1065,  0.0726,  0.1506, -0.0022,\n",
      "         0.0810,  0.1076, -0.0461,  0.0316,  0.0722,  0.1250, -0.0408, -0.0453,\n",
      "        -0.0365, -0.0094,  0.0347,  0.1195,  0.0436,  0.0472,  0.1248,  0.1215,\n",
      "         0.0575,  0.1344,  0.0852,  0.0969,  0.1473,  0.0945,  0.1485,  0.0343,\n",
      "         0.1006,  0.0682,  0.0993,  0.0383,  0.0480, -0.0283,  0.1290,  0.1088,\n",
      "         0.0406,  0.0245,  0.1169,  0.0127,  0.1705,  0.0895,  0.0543,  0.1127,\n",
      "         0.0735,  0.1680,  0.1258,  0.1257,  0.0833,  0.0037,  0.0247,  0.1838,\n",
      "         0.1516, -0.0166,  0.1465, -0.0082,  0.0080,  0.0658,  0.0287,  0.0492,\n",
      "         0.0097, -0.0167,  0.0197, -0.0391,  0.1392,  0.1284,  0.1717,  0.0928,\n",
      "         0.0248,  0.0464,  0.0353,  0.0188, -0.0144,  0.0257,  0.0087,  0.1551,\n",
      "         0.1612,  0.1056,  0.0452, -0.0145,  0.0679,  0.0125,  0.1159,  0.0514,\n",
      "        -0.0311,  0.0345,  0.1097,  0.0418,  0.1094, -0.0240,  0.0693,  0.1913,\n",
      "         0.1535,  0.1408,  0.0448,  0.0909, -0.0241, -0.0178,  0.1751,  0.0370,\n",
      "         0.1084,  0.1660,  0.0629,  0.0218,  0.1224,  0.0698,  0.0583,  0.0412,\n",
      "         0.0373, -0.0100, -0.0860,  0.0540, -0.0300, -0.0474,  0.0796,  0.0896,\n",
      "        -0.0233,  0.0969, -0.0363, -0.0463,  0.0180, -0.0661, -0.0120,  0.0211,\n",
      "        -0.0911,  0.0237,  0.0066, -0.0295, -0.0618,  0.0166, -0.0793, -0.0746,\n",
      "        -0.0349, -0.0196, -0.0435, -0.0087,  0.0241,  0.0635,  0.0051, -0.0002,\n",
      "         0.0036, -0.0214,  0.0548,  0.0919, -0.0482, -0.0169, -0.1116, -0.0429,\n",
      "        -0.0761,  0.0640, -0.0311, -0.0221,  0.0173, -0.0435,  0.0362, -0.0205,\n",
      "         0.0504, -0.0939,  0.0607,  0.0848, -0.0210,  0.1099,  0.0368,  0.0103,\n",
      "         0.0040, -0.0237, -0.0278, -0.0509, -0.0401,  0.0165, -0.0203,  0.0363,\n",
      "        -0.0179,  0.0323, -0.0035,  0.0935,  0.0106,  0.0036,  0.0616, -0.0533,\n",
      "        -0.0265, -0.0372, -0.1205,  0.0103,  0.0949, -0.0275,  0.0042, -0.0407,\n",
      "         0.0235,  0.0306,  0.0546, -0.0129, -0.0288,  0.0912,  0.0094,  0.0532,\n",
      "         0.0696, -0.0246, -0.0618, -0.0389, -0.0109, -0.0407, -0.0734, -0.0356,\n",
      "         0.0689,  0.0341, -0.0257,  0.0761,  0.0192,  0.0438, -0.0360,  0.0848,\n",
      "        -0.0449, -0.0695,  0.0698,  0.0505,  0.0696,  0.0834, -0.0410, -0.0604,\n",
      "        -0.0624,  0.1318,  0.0315, -0.0210,  0.0224,  0.0589, -0.0434, -0.0095,\n",
      "        -0.0817,  0.0062, -0.0552,  0.0340,  0.0155, -0.0655, -0.0051, -0.0723,\n",
      "         0.0365,  0.1029,  0.0559,  0.0778,  0.0323, -0.0116,  0.0880,  0.0487,\n",
      "         0.0385, -0.0719, -0.0986, -0.0246,  0.0391, -0.0515,  0.0035, -0.0216,\n",
      "        -0.0294, -0.0685,  0.0281, -0.0879,  0.0791, -0.0247, -0.0433, -0.0530,\n",
      "         0.0406, -0.0838, -0.0608, -0.1182, -0.0303,  0.0044,  0.0439,  0.0522,\n",
      "         0.0257, -0.0761, -0.0061,  0.0262,  0.0198, -0.0036, -0.0757, -0.0487,\n",
      "        -0.0533,  0.0570,  0.0483, -0.0218,  0.0346, -0.0852, -0.0144, -0.0598,\n",
      "         0.0456, -0.0009, -0.0847,  0.0232,  0.0138, -0.0715, -0.0672, -0.0108,\n",
      "        -0.0969,  0.0238,  0.0327,  0.0025,  0.0066,  0.0848,  0.0308, -0.0400,\n",
      "         0.0990,  0.0205,  0.0382,  0.0582, -0.0196, -0.0388, -0.0308,  0.0283,\n",
      "         0.0208,  0.1033,  0.0680,  0.0121, -0.0287,  0.0673,  0.0340,  0.0276,\n",
      "        -0.0207,  0.0541, -0.0725, -0.0823,  0.0371,  0.0535, -0.0276, -0.0034,\n",
      "        -0.0860, -0.0587, -0.0624,  0.0988,  0.1029,  0.1029, -0.0235,  0.0635,\n",
      "         0.0397,  0.1236, -0.0450, -0.0725,  0.0592, -0.0196, -0.0041, -0.0782,\n",
      "         0.0974, -0.0407,  0.0326, -0.0538,  0.0368, -0.0480, -0.0900, -0.0771,\n",
      "        -0.0130, -0.0471,  0.0646, -0.0545, -0.0593, -0.0262,  0.1030,  0.0169,\n",
      "        -0.1098, -0.0477,  0.0383, -0.0336,  0.0651, -0.0511,  0.0604,  0.0740,\n",
      "         0.1266,  0.0263,  0.1527,  0.0275,  0.0349,  0.0665,  0.0289, -0.0052,\n",
      "         0.0882,  0.0226,  0.0629,  0.0062,  0.1775,  0.0386, -0.0163,  0.1220,\n",
      "         0.1047,  0.1088,  0.1129,  0.1048,  0.0565,  0.0088,  0.0823,  0.0805,\n",
      "         0.0207,  0.1159,  0.1238,  0.0532,  0.0592,  0.1668, -0.0229,  0.0963,\n",
      "        -0.0391,  0.0443,  0.1116,  0.0035,  0.1576,  0.0503,  0.1388,  0.0072,\n",
      "         0.1206,  0.0832, -0.0121,  0.1466,  0.0285,  0.1243,  0.0189,  0.0018,\n",
      "         0.1498,  0.1813,  0.0396,  0.0917,  0.0675, -0.0189,  0.0490, -0.0215,\n",
      "         0.1749,  0.1388,  0.1149, -0.0254,  0.0607,  0.0116,  0.0067, -0.0358,\n",
      "         0.0309,  0.1385,  0.1031,  0.0313,  0.0261,  0.0768,  0.1504,  0.0642,\n",
      "         0.1160,  0.0020,  0.1747,  0.1022, -0.0028, -0.0405,  0.1264, -0.0307,\n",
      "         0.0572,  0.1353,  0.0687,  0.0602,  0.1224,  0.1564,  0.0817,  0.0471,\n",
      "         0.0543,  0.1485,  0.0784,  0.0570,  0.0588,  0.0688,  0.1526,  0.0693,\n",
      "         0.0974,  0.0184,  0.0028, -0.0102,  0.0527,  0.1549,  0.0455,  0.0716,\n",
      "         0.0056,  0.1408,  0.0760,  0.1588,  0.0689,  0.0954,  0.0787,  0.1165,\n",
      "         0.1652,  0.1109,  0.0447,  0.1032,  0.0007,  0.0179,  0.1115,  0.1092,\n",
      "         0.1783,  0.1283,  0.0818,  0.1496, -0.0233,  0.0532,  0.0690, -0.0056],\n",
      "       requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[-0.0134,  0.0061,  0.0488,  ..., -0.0270, -0.0553, -0.0503],\n",
      "        [-0.0080,  0.0655,  0.0569,  ...,  0.0866, -0.0215, -0.0356],\n",
      "        [-0.0098,  0.1085,  0.0957,  ...,  0.1203,  0.1519,  0.0653],\n",
      "        ...,\n",
      "        [ 0.0904, -0.0488,  0.0047,  ...,  0.0853,  0.0447, -0.0197],\n",
      "        [ 0.1827,  0.0335, -0.0571,  ..., -0.0644,  0.0067,  0.0149],\n",
      "        [ 0.0391,  0.0591, -0.2267,  ..., -0.0969,  0.0845, -0.0070]],\n",
      "       requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([-6.9025e-02, -7.4511e-02,  4.2346e-02, -1.0695e-02,  3.1025e-02,\n",
      "         5.6746e-02,  4.9658e-02,  3.7561e-02,  1.3700e-02,  3.2148e-02,\n",
      "         1.9357e-02,  3.9780e-02, -1.8346e-02,  5.5264e-05, -2.7935e-02,\n",
      "        -2.8306e-03,  1.7311e-02, -2.2607e-02, -2.1784e-02, -6.0063e-02],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger_Pretrained(afr_weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 2.535 | Train Acc: 20.10%\n",
      "\t Val. Loss: 2.328 |  Val. Acc: 28.37%\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 2.051 | Train Acc: 43.47%\n",
      "\t Val. Loss: 1.915 |  Val. Acc: 56.55%\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 1.576 | Train Acc: 66.46%\n",
      "\t Val. Loss: 1.461 |  Val. Acc: 68.66%\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 1.106 | Train Acc: 79.26%\n",
      "\t Val. Loss: 1.049 |  Val. Acc: 80.48%\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.726 | Train Acc: 87.34%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 85.85%\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.470 | Train Acc: 91.83%\n",
      "\t Val. Loss: 0.556 |  Val. Acc: 88.73%\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.315 | Train Acc: 94.16%\n",
      "\t Val. Loss: 0.446 |  Val. Acc: 89.96%\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.225 | Train Acc: 95.62%\n",
      "\t Val. Loss: 0.382 |  Val. Acc: 90.74%\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.164 | Train Acc: 97.09%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 91.42%\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.125 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 91.83%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.301 |  Test Acc: 92.31%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
