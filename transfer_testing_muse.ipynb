{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "du_embed = {}\n",
    "with open(\"wiki.multi.nl.vec\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        vector = list(map(float, tokens[1:]))\n",
    "        tensor_vector = torch.FloatTensor(vector)\n",
    "        du_embed[word] = tensor_vector\n",
    "embedding_dim = len(du_embed['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1703\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_dev_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  12264\n",
      "Tagged words in test set: 185999\n",
      "****************************************\n",
      "Total sentences in dataset: 13700\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_dev_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Found:  158\n",
      "Words Missing:  4519\n"
     ]
    }
   ],
   "source": [
    "matrix_len = len(train_du.vocab.itos)\n",
    "weights_matrix = torch.zeros((matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        weights_matrix[i] = du_embed[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        weights_matrix[i] = torch.empty(embedding_dim, ).normal_()\n",
    "        words_missing += 1\n",
    "print(\"Words Found: \", words_found)\n",
    "print(\"Words Missing: \", words_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    input_dim, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(input_dim, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, input_dim, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger_PreEmbed(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, weights_matrix, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding, input_dim, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger_PreEmbed(weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 1.699 | Train Acc: 49.08%\n",
      "\t Val. Loss: 1.303 |  Val. Acc: 60.90%\n",
      "Epoch: 02 | Epoch Time: 1m 31s\n",
      "\tTrain Loss: 0.890 | Train Acc: 73.40%\n",
      "\t Val. Loss: 1.012 |  Val. Acc: 70.18%\n",
      "Epoch: 03 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.612 | Train Acc: 82.17%\n",
      "\t Val. Loss: 0.862 |  Val. Acc: 75.02%\n",
      "Epoch: 04 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.464 | Train Acc: 86.62%\n",
      "\t Val. Loss: 0.808 |  Val. Acc: 77.57%\n",
      "Epoch: 05 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.374 | Train Acc: 89.35%\n",
      "\t Val. Loss: 0.773 |  Val. Acc: 79.32%\n",
      "Epoch: 06 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.315 | Train Acc: 91.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 80.20%\n",
      "Epoch: 07 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.273 | Train Acc: 92.20%\n",
      "\t Val. Loss: 0.796 |  Val. Acc: 80.17%\n",
      "Epoch: 08 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.241 | Train Acc: 93.10%\n",
      "\t Val. Loss: 0.788 |  Val. Acc: 81.01%\n",
      "Epoch: 09 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.215 | Train Acc: 93.87%\n",
      "\t Val. Loss: 0.784 |  Val. Acc: 81.25%\n",
      "Epoch: 10 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.193 | Train Acc: 94.50%\n",
      "\t Val. Loss: 0.807 |  Val. Acc: 81.14%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.164 |  Test Acc: 95.52%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[-0.9891,  0.0520,  0.2516,  ..., -0.5476, -1.3248,  0.7580],\n",
      "        [-1.9092,  0.9414,  0.1942,  ...,  0.6298,  0.1553,  1.1409],\n",
      "        [ 0.0358,  0.8690,  0.1499,  ...,  0.2708, -0.0618,  1.2390],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.2180, -0.0073,  0.0912,  ...,  0.0290, -0.1063,  0.0969],\n",
      "        [-0.0879,  0.0067, -0.0118,  ..., -0.0657, -0.0923,  0.0284],\n",
      "        [ 0.1929, -0.1973,  0.0762,  ..., -0.3319, -0.0082,  0.0360],\n",
      "        ...,\n",
      "        [-0.0722, -0.0550, -0.0291,  ..., -0.0740, -0.2051, -0.0015],\n",
      "        [-0.0739,  0.1168, -0.1411,  ..., -0.0786,  0.0053, -0.0652],\n",
      "        [ 0.2023, -0.1211,  0.1133,  ...,  0.0076,  0.1061,  0.0340]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[-8.4739e-02,  1.7700e-01, -1.3616e-01,  ...,  1.7468e-01,\n",
      "         -5.9369e-03, -1.2195e-02],\n",
      "        [-2.3796e-03,  1.5925e-01, -4.8942e-02,  ...,  1.4185e-01,\n",
      "         -2.9837e-02, -4.5384e-02],\n",
      "        [-5.0176e-02,  3.8202e-02, -2.9902e-02,  ...,  5.0028e-02,\n",
      "         -2.1381e-01,  8.9225e-02],\n",
      "        ...,\n",
      "        [ 1.3963e-01,  7.1609e-02,  5.3498e-02,  ...,  1.1016e-01,\n",
      "          1.2315e-02,  8.7107e-02],\n",
      "        [-1.0071e-01,  1.9247e-01,  9.9012e-03,  ...,  1.5253e-01,\n",
      "         -7.7463e-02,  2.1364e-02],\n",
      "        [ 7.7592e-02,  1.5385e-01,  2.1056e-01,  ...,  3.2077e-02,\n",
      "         -9.9275e-05, -4.5497e-02]], requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.1938,  0.1306,  0.0277,  0.0940,  0.1353,  0.1021,  0.1990,  0.0089,\n",
      "         0.0757,  0.1704, -0.0186,  0.0680,  0.1351,  0.1296,  0.0028,  0.1210,\n",
      "         0.1243,  0.1681,  0.0611,  0.1772,  0.0520,  0.0061,  0.1170,  0.0701,\n",
      "        -0.0053,  0.1184,  0.0973,  0.0224,  0.0409,  0.0545,  0.1044,  0.1451,\n",
      "         0.1427,  0.1780,  0.1008,  0.0981,  0.1695,  0.1134,  0.0538,  0.0656,\n",
      "        -0.0111,  0.0335,  0.0978,  0.1272,  0.0495,  0.0500,  0.1847,  0.1111,\n",
      "         0.1909,  0.0773,  0.0495,  0.1486, -0.0289,  0.1104, -0.0067,  0.2521,\n",
      "         0.1460,  0.0494,  0.1878, -0.0113,  0.2131,  0.1926,  0.0274,  0.2378,\n",
      "         0.0520,  0.2292,  0.1676,  0.1495,  0.2254,  0.0900,  0.0790,  0.1311,\n",
      "         0.0706,  0.1502, -0.0017,  0.0559,  0.1825,  0.1316,  0.1323,  0.1721,\n",
      "         0.0699,  0.1683,  0.1706,  0.1093,  0.0845,  0.1458,  0.1437,  0.1183,\n",
      "         0.1786,  0.2245,  0.1121, -0.0959,  0.0890,  0.0182,  0.1303,  0.0383,\n",
      "         0.1330,  0.1602, -0.0144, -0.0293,  0.0314,  0.0390,  0.1009,  0.1494,\n",
      "         0.0582,  0.0741,  0.0470,  0.1623,  0.0905,  0.0930,  0.1851,  0.0423,\n",
      "         0.0962,  0.1652,  0.0772,  0.0620,  0.1711,  0.0661,  0.0074, -0.0263,\n",
      "         0.1283,  0.1837,  0.1222,  0.1636,  0.0633, -0.0077,  0.0624,  0.1209,\n",
      "         0.0338,  0.1250, -0.0261, -0.0741,  0.0625,  0.0553,  0.0289,  0.1410,\n",
      "         0.0399, -0.0897, -0.0372, -0.1221, -0.0530, -0.0791,  0.0622,  0.0584,\n",
      "        -0.0148,  0.0291,  0.0483, -0.1533, -0.1081, -0.0298, -0.0317,  0.0559,\n",
      "         0.0367, -0.0602, -0.1082, -0.0044,  0.1726,  0.0817,  0.0562,  0.0759,\n",
      "        -0.0900, -0.0765, -0.0628, -0.0781,  0.0203,  0.0150, -0.0178,  0.0157,\n",
      "        -0.0326, -0.0244, -0.0501, -0.0745,  0.0514,  0.0763, -0.1037,  0.0119,\n",
      "         0.0393, -0.0444,  0.0040,  0.0530, -0.0023, -0.0297,  0.0667, -0.0947,\n",
      "         0.0948,  0.0497, -0.1050,  0.0476, -0.1286,  0.0306,  0.1363, -0.1322,\n",
      "         0.0205, -0.1118, -0.0577, -0.0648, -0.0902, -0.1754,  0.1032, -0.0558,\n",
      "         0.0329,  0.0008,  0.1537,  0.0054, -0.0785,  0.0601, -0.0478, -0.1005,\n",
      "         0.1007, -0.0629, -0.0090,  0.0205,  0.0940,  0.0422,  0.0872,  0.0840,\n",
      "         0.0869, -0.0252,  0.0519, -0.0111, -0.0535,  0.1303,  0.0163, -0.1321,\n",
      "        -0.1068, -0.0019,  0.1528,  0.0612,  0.1024, -0.0302, -0.0562, -0.0060,\n",
      "         0.0753, -0.0302,  0.0157,  0.0730, -0.0871, -0.0444,  0.0110,  0.1051,\n",
      "         0.0379, -0.0623, -0.0555, -0.0063, -0.0489, -0.0465,  0.0547,  0.1131,\n",
      "         0.0649, -0.1409, -0.0049,  0.0107,  0.0974,  0.0303,  0.0198,  0.0281,\n",
      "        -0.0414,  0.1282,  0.0751, -0.0178, -0.0964,  0.0715,  0.0318,  0.0442,\n",
      "        -0.0226,  0.0541,  0.0615,  0.0694, -0.1114,  0.0584,  0.0080, -0.0651,\n",
      "        -0.0437,  0.0786,  0.0335, -0.0708, -0.0680, -0.0429,  0.0034,  0.0487,\n",
      "         0.0233, -0.0427, -0.0195, -0.0232,  0.0701, -0.0590,  0.0869,  0.0712,\n",
      "        -0.0137, -0.0495, -0.0664,  0.0477, -0.1148, -0.0826, -0.0060,  0.0723,\n",
      "        -0.0023,  0.0566, -0.0884,  0.0097,  0.0863,  0.0108, -0.0282, -0.0501,\n",
      "         0.0346, -0.0284, -0.0666,  0.1117, -0.0869,  0.0215,  0.0667, -0.0762,\n",
      "        -0.0720,  0.0196,  0.0184, -0.1030, -0.0636,  0.0765, -0.0759,  0.0501,\n",
      "        -0.0919,  0.0589,  0.0196, -0.0398, -0.0392, -0.0007, -0.0457,  0.0436,\n",
      "        -0.0858,  0.0502, -0.1076,  0.0363, -0.0967,  0.0594, -0.0548,  0.0671,\n",
      "        -0.0123,  0.0392,  0.0917,  0.0163, -0.0143,  0.1001,  0.0425, -0.0466,\n",
      "         0.0249, -0.0310,  0.0101,  0.0180,  0.0242,  0.0753,  0.0422, -0.0694,\n",
      "         0.0324,  0.0615,  0.0430, -0.0232,  0.0057,  0.1194,  0.0679, -0.0586,\n",
      "        -0.0574, -0.0731, -0.0882, -0.0494, -0.0534,  0.0527,  0.0714,  0.0121,\n",
      "        -0.1071,  0.0470,  0.1126,  0.0961,  0.1093,  0.0882, -0.0903, -0.0989,\n",
      "         0.0053,  0.0395,  0.1158,  0.0338, -0.0322,  0.0896,  0.0506,  0.1130,\n",
      "         0.0562,  0.1403,  0.0310,  0.1238,  0.0695,  0.1642,  0.0653,  0.0118,\n",
      "         0.0772,  0.0932,  0.1146,  0.1196,  0.0730,  0.1335,  0.0137,  0.1561,\n",
      "         0.2159,  0.1034, -0.0139,  0.0865,  0.0940,  0.1182,  0.1330,  0.1865,\n",
      "         0.0528,  0.1450,  0.1962,  0.1353,  0.1152,  0.1257,  0.1167,  0.0379,\n",
      "         0.1356,  0.1928,  0.0610,  0.0833,  0.0428, -0.0028,  0.1826,  0.0759,\n",
      "         0.0923,  0.1455,  0.1842,  0.1350,  0.1386,  0.0161,  0.1606,  0.1707,\n",
      "         0.1333,  0.1381,  0.1013,  0.0242, -0.0559,  0.0580,  0.1153,  0.1312,\n",
      "         0.0648,  0.1436,  0.1788,  0.1174,  0.1691,  0.1543,  0.1226,  0.2193,\n",
      "         0.1730,  0.0953,  0.1441,  0.0560,  0.0572,  0.1985,  0.0708,  0.1227,\n",
      "         0.1194,  0.1787,  0.1238,  0.1544,  0.1150,  0.1641,  0.0773,  0.2074,\n",
      "         0.0187,  0.2510,  0.1454,  0.1229,  0.0644,  0.1096,  0.1396,  0.0717,\n",
      "         0.1663,  0.1338,  0.0336,  0.1693,  0.0861,  0.1431,  0.1629,  0.0973,\n",
      "         0.1053,  0.1978,  0.0301, -0.0026,  0.1684,  0.0156,  0.0652,  0.1408,\n",
      "         0.0427,  0.2077,  0.1155,  0.0818,  0.1459,  0.0739,  0.1934,  0.1512,\n",
      "         0.0522,  0.1845,  0.0817,  0.1244,  0.2154,  0.1380,  0.0881,  0.1027,\n",
      "         0.0478,  0.0788,  0.0869,  0.1844,  0.1610,  0.1363,  0.1605,  0.1234],\n",
      "       requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.0988,  0.1150,  0.1243,  0.1984, -0.0197,  0.1275,  0.1504, -0.0125,\n",
      "         0.0144,  0.1557,  0.0883,  0.1221,  0.0653,  0.0873,  0.0334,  0.0621,\n",
      "         0.0874,  0.1388,  0.0490,  0.1727,  0.1872,  0.0737,  0.1280,  0.1887,\n",
      "         0.1322,  0.1384,  0.1934,  0.1310,  0.1451,  0.1126,  0.1227,  0.1171,\n",
      "         0.0613,  0.2120,  0.0675,  0.2109,  0.1600,  0.1181,  0.0206, -0.0137,\n",
      "         0.0838,  0.1391,  0.0733,  0.1437,  0.0853,  0.0512,  0.1559,  0.0375,\n",
      "         0.1679,  0.0878,  0.1712,  0.0087,  0.0081,  0.1972,  0.0308,  0.1016,\n",
      "         0.0004,  0.0166,  0.0881,  0.1037,  0.1288,  0.0749,  0.1286,  0.1026,\n",
      "         0.0009,  0.0932,  0.1313,  0.1080,  0.1047,  0.1278,  0.0249,  0.0455,\n",
      "         0.0109,  0.1606,  0.0789,  0.1454,  0.0857,  0.0872,  0.1437,  0.1945,\n",
      "        -0.0081,  0.2128,  0.1357,  0.0354,  0.0125,  0.0914,  0.0672,  0.1190,\n",
      "         0.1608,  0.1060,  0.1110,  0.0261,  0.2143,  0.0755,  0.0944,  0.0918,\n",
      "         0.1062,  0.1141,  0.1044,  0.0927,  0.0325,  0.1387,  0.0064,  0.1919,\n",
      "         0.1305,  0.1919,  0.1604,  0.0380,  0.1572,  0.1265,  0.0406,  0.0064,\n",
      "         0.0403,  0.0098,  0.0251,  0.0378,  0.1605,  0.0536,  0.0358, -0.0450,\n",
      "        -0.0107,  0.0728,  0.0849,  0.1571,  0.1369,  0.0015,  0.1928,  0.1536,\n",
      "         0.0400,  0.0234,  0.0695, -0.1099,  0.0041, -0.0153,  0.0161,  0.1193,\n",
      "         0.0102, -0.1029, -0.0074, -0.0507, -0.0290, -0.1136,  0.1184,  0.0563,\n",
      "        -0.0883,  0.0337,  0.0857, -0.0268, -0.0664,  0.0757, -0.0707, -0.0226,\n",
      "        -0.0445, -0.0025,  0.0044, -0.0555,  0.0226,  0.0657, -0.0405, -0.0020,\n",
      "        -0.0009,  0.0277,  0.0168, -0.1392, -0.0629,  0.0492, -0.0686,  0.0166,\n",
      "         0.0061,  0.1131, -0.0681, -0.0581,  0.0356,  0.0198, -0.0886,  0.0204,\n",
      "        -0.0373, -0.1400, -0.0417,  0.1062,  0.0161, -0.1308, -0.0064, -0.1026,\n",
      "         0.1116,  0.0592, -0.1649,  0.0810, -0.1067, -0.0869, -0.0169, -0.1010,\n",
      "         0.0205, -0.0998, -0.1328, -0.0837,  0.0296, -0.1679,  0.0634, -0.0807,\n",
      "        -0.0284,  0.0449,  0.0312,  0.0338, -0.0474,  0.1246,  0.0277, -0.1210,\n",
      "         0.0406, -0.0717, -0.0237,  0.1209, -0.0690,  0.0743, -0.0337, -0.0264,\n",
      "         0.0675,  0.0155,  0.1224,  0.0924, -0.0248,  0.0437, -0.0340, -0.1301,\n",
      "         0.0206,  0.0109,  0.1287,  0.0693, -0.0492,  0.1113,  0.0906, -0.0739,\n",
      "        -0.0326, -0.1230,  0.0084, -0.0774, -0.0959, -0.0043,  0.0439,  0.0886,\n",
      "         0.0547, -0.0625, -0.0016,  0.0534, -0.0951, -0.0469,  0.0837,  0.0354,\n",
      "         0.1518, -0.1530, -0.0601,  0.0079, -0.0274, -0.0026,  0.0103, -0.0668,\n",
      "        -0.0289,  0.0524,  0.0702,  0.0805,  0.0010, -0.0025,  0.0699, -0.0524,\n",
      "         0.0761, -0.0311,  0.0912,  0.0884,  0.0049,  0.0660, -0.0520,  0.0606,\n",
      "         0.0431,  0.0438,  0.0271, -0.0436, -0.0091, -0.0337,  0.0172, -0.0418,\n",
      "         0.0752, -0.0186, -0.0903,  0.1158,  0.0682, -0.0930,  0.0615,  0.0388,\n",
      "         0.1174, -0.0427, -0.0112,  0.0378, -0.0970, -0.0984, -0.0149,  0.0521,\n",
      "         0.0879, -0.0665, -0.0778, -0.0121,  0.0782, -0.0070,  0.0238,  0.0697,\n",
      "         0.0259, -0.0100, -0.0774,  0.0513, -0.0752,  0.0242,  0.1055,  0.0142,\n",
      "        -0.1201,  0.0940, -0.0164, -0.0690,  0.0454, -0.0680, -0.0976,  0.0337,\n",
      "        -0.0980, -0.0017, -0.0137,  0.0292,  0.0096, -0.0800, -0.0635, -0.0418,\n",
      "        -0.0691,  0.0803, -0.1103, -0.0640,  0.0249,  0.0420, -0.0771, -0.0671,\n",
      "        -0.0724,  0.0041,  0.0455,  0.0768,  0.0395,  0.0052,  0.0945, -0.0424,\n",
      "         0.0345,  0.0141,  0.1075,  0.0108, -0.0244,  0.0558, -0.0050,  0.0579,\n",
      "         0.1053,  0.0426,  0.0734,  0.0543,  0.0777,  0.0721,  0.1069,  0.0584,\n",
      "         0.0644,  0.0009, -0.0241, -0.0493,  0.0544,  0.0229,  0.0370, -0.1195,\n",
      "        -0.0060,  0.0838,  0.1139, -0.0167,  0.0207,  0.0837, -0.0902,  0.0292,\n",
      "         0.1153, -0.0270,  0.0432,  0.0483,  0.0010,  0.0711,  0.0446, -0.0169,\n",
      "         0.0732,  0.1940,  0.0524,  0.1658,  0.0677,  0.0682,  0.2233,  0.1247,\n",
      "         0.1485,  0.2280,  0.1727,  0.2160,  0.1144,  0.1656, -0.0409,  0.1383,\n",
      "         0.1609,  0.1484,  0.0032,  0.1086,  0.2252,  0.0195,  0.1581,  0.0311,\n",
      "         0.1180,  0.0788,  0.0881,  0.1839,  0.1833,  0.0050,  0.0582,  0.0366,\n",
      "         0.1013,  0.0951,  0.0832,  0.1853,  0.0976,  0.1085,  0.0624,  0.0491,\n",
      "         0.0969,  0.0494,  0.1727,  0.0777,  0.1197,  0.1448,  0.1120,  0.1466,\n",
      "         0.1999,  0.1832,  0.0793,  0.0742,  0.1108,  0.1072,  0.1485,  0.1843,\n",
      "         0.0590,  0.0510,  0.1235,  0.1779,  0.1067,  0.2253,  0.0675,  0.1689,\n",
      "         0.0227,  0.1391,  0.1239,  0.1746,  0.1149,  0.1192,  0.0917,  0.2077,\n",
      "         0.1294,  0.0419,  0.1329,  0.0978,  0.0682,  0.1809,  0.0683,  0.1067,\n",
      "         0.1248,  0.1965,  0.1046,  0.0619,  0.0407,  0.0306,  0.0848,  0.0584,\n",
      "         0.1128,  0.0988,  0.0253,  0.0811,  0.0984,  0.0419,  0.2234,  0.1686,\n",
      "         0.0700,  0.0632,  0.0139,  0.1412,  0.1241,  0.0925,  0.1201,  0.1762,\n",
      "         0.0593,  0.1723,  0.0929,  0.1406,  0.0559,  0.1480,  0.1996,  0.0113,\n",
      "         0.0613,  0.0559,  0.0870,  0.0721,  0.1551,  0.0960,  0.0163,  0.0656,\n",
      "         0.0267,  0.1424,  0.1242,  0.1551,  0.0432, -0.0308,  0.0335,  0.0635],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.0583, -0.0369, -0.1034,  ...,  0.1613,  0.0138, -0.1736],\n",
      "        [-0.1683, -0.1507, -0.1303,  ..., -0.0322,  0.0473,  0.2394],\n",
      "        [ 0.2730, -0.2635,  0.0465,  ..., -0.0104, -0.2417, -0.0199],\n",
      "        ...,\n",
      "        [ 0.1826, -0.1018,  0.0905,  ..., -0.1527, -0.0013,  0.1853],\n",
      "        [-0.0766,  0.0787, -0.1835,  ..., -0.0064, -0.1252,  0.0172],\n",
      "        [-0.0876, -0.0027, -0.0834,  ...,  0.0783, -0.0056,  0.0745]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[ 6.9922e-03,  2.3216e-01, -6.1771e-02,  ..., -4.5010e-02,\n",
      "         -7.6024e-02,  1.0657e-01],\n",
      "        [-8.2709e-02, -1.8866e-02,  1.6285e-01,  ..., -1.6750e-01,\n",
      "          7.1719e-02,  8.0355e-02],\n",
      "        [ 5.0365e-02, -3.1204e-02,  8.9674e-03,  ..., -1.1693e-04,\n",
      "          1.1152e-01, -1.3750e-02],\n",
      "        ...,\n",
      "        [-5.8046e-02,  7.9164e-02,  1.1044e-01,  ..., -3.2882e-02,\n",
      "          2.4343e-02,  4.0821e-02],\n",
      "        [-3.0867e-02,  4.2751e-02,  1.2032e-01,  ..., -7.8592e-02,\n",
      "          1.1537e-01,  4.0342e-02],\n",
      "        [-1.8852e-02,  8.1523e-02,  1.6947e-01,  ...,  2.1236e-02,\n",
      "          4.8270e-02, -1.2536e-01]], requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([ 5.5874e-02,  1.0130e-01,  1.2553e-01,  4.4129e-03,  1.1739e-01,\n",
      "        -7.2991e-03,  4.8819e-02,  1.1496e-01,  7.5150e-02,  1.4733e-02,\n",
      "         1.2508e-01,  8.2951e-02,  5.4772e-02,  7.7985e-03,  4.3911e-02,\n",
      "         7.2729e-02,  7.5304e-02,  4.3583e-02,  5.9280e-02, -1.4136e-02,\n",
      "         1.1156e-01,  7.1527e-02,  1.2311e-01, -1.2849e-02,  2.4167e-02,\n",
      "         7.1890e-02,  5.8988e-02,  7.7081e-02, -2.9094e-02, -3.9592e-02,\n",
      "         5.3621e-02,  5.4848e-02,  6.4797e-02,  1.6494e-01,  2.1880e-01,\n",
      "         2.1438e-02,  1.2744e-02, -1.3236e-02,  1.4319e-01,  6.9225e-02,\n",
      "         6.0721e-03,  1.1744e-02,  9.9565e-02,  1.6513e-01,  5.4209e-02,\n",
      "         1.4536e-01,  1.0166e-01,  1.2464e-01,  1.3446e-01,  7.2268e-03,\n",
      "         6.5884e-02,  1.6523e-01,  9.1631e-02,  6.7125e-02,  5.5173e-02,\n",
      "         1.2209e-01,  1.8787e-01,  1.3947e-01,  5.8959e-02,  1.4831e-01,\n",
      "         5.6879e-02,  1.1597e-01,  7.8351e-02,  1.2793e-01,  6.4796e-02,\n",
      "         5.1058e-02,  1.1264e-01,  4.9881e-02,  2.1136e-01,  1.1445e-01,\n",
      "         9.3741e-02,  4.1171e-02,  9.7124e-02,  6.1547e-02,  3.2853e-02,\n",
      "         9.3227e-02,  7.3783e-02, -5.0514e-02,  5.7619e-02,  2.1085e-01,\n",
      "        -1.6751e-02, -2.7888e-03,  6.5682e-02,  8.5835e-02,  6.8265e-02,\n",
      "         1.1647e-01,  7.6317e-02,  1.4029e-01,  3.0599e-02,  1.2870e-01,\n",
      "         2.6934e-02,  1.1462e-01, -6.5257e-03,  1.4400e-01,  1.4074e-01,\n",
      "         1.1061e-01,  1.2074e-01, -1.8756e-02,  1.4866e-01,  1.1540e-01,\n",
      "         1.0195e-01,  9.3148e-02,  9.9076e-02,  4.3905e-02,  1.6344e-01,\n",
      "        -3.0898e-02,  1.6375e-01,  8.8297e-02,  6.0239e-02,  1.2838e-01,\n",
      "         3.1377e-02,  1.2624e-01,  1.3172e-03,  8.7608e-02,  2.9166e-02,\n",
      "         1.1751e-01,  1.9958e-02,  3.3805e-02,  3.7372e-02,  1.6258e-01,\n",
      "         1.7285e-01,  9.1104e-02,  1.4036e-01,  1.0450e-01,  1.0617e-01,\n",
      "         1.3292e-01,  9.9286e-02,  2.3312e-02, -1.0072e-01, -1.3964e-01,\n",
      "        -4.6639e-02, -1.5381e-02,  3.0133e-02,  2.2389e-02,  3.2084e-02,\n",
      "         4.5281e-02,  7.4230e-02,  7.9526e-02, -2.3789e-02, -3.9581e-03,\n",
      "        -2.0122e-02,  1.2663e-01, -1.2944e-01,  1.2038e-01,  3.8493e-02,\n",
      "         1.1334e-01,  4.7892e-03, -1.9615e-02, -6.7095e-02, -4.2910e-02,\n",
      "         5.2437e-02,  8.1049e-02, -2.8293e-02, -7.4081e-02, -3.5824e-02,\n",
      "         9.1590e-02, -1.2201e-02, -4.1906e-02, -3.2054e-02, -7.0835e-02,\n",
      "        -7.3604e-02, -2.0754e-02, -4.5198e-02,  6.1103e-02, -7.7505e-03,\n",
      "        -3.1085e-02, -3.2818e-02,  4.2082e-02,  1.8120e-02,  4.8109e-02,\n",
      "         7.7943e-02, -6.0838e-02,  5.7990e-02,  6.4637e-03, -7.6294e-02,\n",
      "         3.7234e-02,  2.6309e-03,  3.1895e-02,  2.7161e-02, -5.2849e-02,\n",
      "        -2.6078e-02,  5.2848e-03,  4.1887e-02,  5.5491e-02, -1.1566e-02,\n",
      "        -4.9910e-02,  6.7383e-02, -9.9049e-03,  1.6494e-02,  1.1320e-01,\n",
      "        -9.8518e-02, -8.5747e-02, -1.6590e-02, -5.6484e-02,  1.5214e-02,\n",
      "        -4.3331e-02, -1.2044e-01, -7.5458e-02, -2.7995e-03, -1.0840e-03,\n",
      "         8.4081e-02, -2.0098e-04,  1.1225e-01, -5.1634e-02, -1.9137e-02,\n",
      "         6.8451e-02, -9.5517e-02,  4.9608e-02, -8.9387e-02,  6.8676e-02,\n",
      "         4.0013e-03,  1.8731e-02, -7.5478e-02,  6.3380e-02, -4.5968e-02,\n",
      "         3.7287e-02,  6.0217e-02, -4.4706e-02,  9.3808e-02, -1.0290e-01,\n",
      "         5.6193e-02, -3.9503e-02, -4.7677e-03, -5.4633e-02,  6.0146e-02,\n",
      "         1.1080e-01,  5.5854e-02,  3.1179e-02, -2.1741e-02,  4.9144e-02,\n",
      "        -1.0077e-02,  6.3255e-02, -9.3315e-02,  6.2012e-02,  4.4852e-02,\n",
      "         1.5181e-02, -1.2688e-04,  3.5468e-02, -4.1158e-02,  6.1563e-02,\n",
      "         1.1946e-01, -2.5300e-02,  5.7690e-02, -1.3312e-02,  5.0765e-02,\n",
      "         6.9572e-02,  2.2252e-03,  4.0499e-03, -6.7316e-02,  1.1027e-01,\n",
      "        -2.5951e-02,  2.4615e-02, -6.6437e-02,  2.1841e-05,  1.8503e-03,\n",
      "         1.8111e-02, -5.6793e-02, -1.4964e-02, -7.0912e-04,  7.8854e-02,\n",
      "         1.9812e-02,  4.2576e-02,  2.3870e-02, -9.0010e-02, -1.0115e-01,\n",
      "        -1.0413e-01, -1.3009e-02, -3.2168e-02, -1.1439e-01, -5.7643e-02,\n",
      "         8.5793e-02, -1.5444e-02,  6.4670e-02,  7.5026e-02,  3.5207e-02,\n",
      "        -3.4632e-02, -2.9830e-02,  5.0142e-02,  5.6831e-02,  7.1533e-02,\n",
      "         9.8808e-02, -2.6468e-02, -2.1400e-02, -3.4138e-03,  8.4884e-02,\n",
      "         4.0165e-02,  6.2940e-02, -2.1976e-02,  7.9059e-03,  3.5652e-02,\n",
      "        -3.6809e-02, -3.8467e-02,  4.1340e-02,  5.1261e-02,  7.9970e-02,\n",
      "         7.6109e-03, -9.7123e-03, -8.8121e-02,  1.2282e-02, -6.4862e-02,\n",
      "         2.8942e-03,  5.4854e-03,  5.2386e-02, -1.8046e-02,  6.4016e-02,\n",
      "         5.7570e-02,  9.4347e-02, -7.6603e-02,  7.6797e-02, -2.3571e-02,\n",
      "        -8.6902e-02, -6.2687e-02,  1.1089e-02, -2.4623e-02, -1.8469e-02,\n",
      "         1.2944e-01, -4.5925e-02,  3.2644e-02, -3.2275e-02,  4.1466e-02,\n",
      "         5.0019e-02, -7.0006e-02,  4.2976e-02, -1.6686e-02,  3.0958e-02,\n",
      "        -8.1453e-02, -5.3555e-02,  8.4965e-03, -1.1874e-01,  2.2466e-02,\n",
      "        -6.1810e-02,  8.9990e-02, -5.1371e-02, -1.4521e-02, -8.2787e-02,\n",
      "         6.5816e-02,  8.6668e-02, -2.4503e-02, -8.2971e-02,  6.8670e-02,\n",
      "        -3.6864e-02,  6.9670e-02, -4.4446e-02, -7.3731e-02, -1.1143e-01,\n",
      "         3.8539e-02, -3.5105e-03, -3.1472e-02, -1.0067e-01, -1.1619e-01,\n",
      "        -3.4326e-02,  1.7776e-02,  9.2282e-02, -6.9758e-02,  5.9956e-02,\n",
      "         7.8788e-03,  1.9402e-02, -5.6309e-02,  6.6647e-03, -1.0851e-01,\n",
      "         5.3285e-02,  9.6834e-02, -1.6761e-02,  4.6597e-02,  6.6699e-02,\n",
      "        -1.1892e-01,  5.6674e-02, -8.5043e-04, -7.9248e-02, -4.3511e-02,\n",
      "         7.3148e-02, -4.7375e-02,  1.7272e-02,  3.5256e-02,  7.2852e-02,\n",
      "         7.6007e-02, -6.5496e-02,  9.1731e-02,  8.5106e-02,  2.9703e-02,\n",
      "         2.4035e-03, -2.4934e-02, -2.7459e-02, -4.9465e-02,  1.3217e-01,\n",
      "         1.1984e-01,  1.3974e-01,  5.0271e-03,  2.4547e-02,  1.1725e-01,\n",
      "         2.9636e-03,  4.2198e-02,  5.4399e-02,  1.3210e-01,  7.2685e-02,\n",
      "         4.2672e-02,  5.4056e-02,  3.3365e-02,  1.2694e-01,  4.3128e-02,\n",
      "         1.0407e-01,  1.4342e-02,  1.2743e-01, -2.7549e-02,  1.1033e-01,\n",
      "         5.0444e-02,  1.7058e-01,  5.3865e-02,  2.9357e-02,  5.1599e-02,\n",
      "         6.3766e-02,  5.3199e-02,  3.8037e-02,  5.5803e-02,  4.2050e-02,\n",
      "         1.9238e-01,  9.9753e-02,  4.0247e-02,  1.7194e-01,  5.4866e-02,\n",
      "         1.7810e-02,  1.1954e-01,  5.1047e-02, -1.3633e-03,  1.3887e-01,\n",
      "        -3.7603e-02,  1.3040e-01,  1.0795e-01,  3.7868e-02,  6.2601e-02,\n",
      "         6.9128e-02,  9.7172e-02,  1.0239e-01, -1.4438e-02,  9.7489e-02,\n",
      "         5.7315e-02,  1.4940e-01,  4.8817e-02,  3.7668e-02,  7.2452e-02,\n",
      "         6.2993e-02,  3.1489e-02,  5.5497e-02,  6.1008e-02,  7.7517e-02,\n",
      "         2.8934e-02,  1.1752e-01,  7.1832e-02,  8.5144e-02,  8.3381e-02,\n",
      "         9.0510e-02,  3.9020e-02,  6.8106e-02,  1.3363e-01,  7.0277e-02,\n",
      "         1.0692e-01,  5.9644e-04,  1.1946e-01, -2.6520e-03,  1.7774e-01,\n",
      "         3.6146e-03,  5.7468e-02,  1.2508e-01,  1.0024e-01,  5.0559e-02,\n",
      "         1.2673e-01, -7.2868e-03,  9.1286e-02,  1.3402e-01,  5.0587e-02,\n",
      "        -4.0173e-02,  1.1600e-01,  5.8086e-02,  1.0960e-01,  8.2932e-02,\n",
      "         1.8634e-01,  4.1007e-02,  2.3119e-02,  1.3800e-01,  1.4132e-01,\n",
      "         1.0488e-01,  6.5218e-02,  7.6887e-02,  6.0288e-04,  3.7523e-02,\n",
      "         9.9042e-02, -2.0989e-02,  5.2767e-02,  1.9499e-01, -1.3601e-02,\n",
      "         1.6835e-01,  3.3616e-03,  7.0438e-02,  9.0146e-02,  7.6033e-02,\n",
      "         9.2439e-02,  1.1241e-01,  8.6189e-02, -2.7131e-02,  5.4851e-02,\n",
      "         4.1480e-02,  1.3670e-01,  5.1436e-02,  1.7170e-01,  1.1840e-01,\n",
      "         1.0777e-02,  5.1239e-02, -1.2335e-02,  1.0767e-01,  6.0454e-02,\n",
      "         2.6525e-02,  3.8782e-02], requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([ 1.8295e-01,  2.1213e-01,  8.0877e-02,  1.5403e-01,  1.0329e-02,\n",
      "         1.3369e-01,  9.6573e-03,  1.0601e-01,  3.3742e-03,  7.0596e-02,\n",
      "         5.3224e-03,  8.0783e-02,  9.5300e-02,  3.8778e-02,  4.8636e-02,\n",
      "         1.3115e-01,  6.8914e-02,  1.2488e-01,  1.1536e-01,  3.4522e-02,\n",
      "         9.8237e-02,  4.9819e-02,  3.3592e-02,  1.8381e-02,  3.1950e-02,\n",
      "         1.1753e-01,  1.8132e-01, -6.8490e-03,  1.4320e-01, -2.9041e-02,\n",
      "         1.0573e-01,  1.6793e-01,  1.8370e-02,  6.8802e-02,  1.6872e-01,\n",
      "         1.8972e-02,  7.5339e-04,  1.0370e-01,  1.5306e-01,  9.1814e-02,\n",
      "         4.4191e-02,  3.8419e-02,  6.4146e-02,  1.4965e-01,  7.6470e-02,\n",
      "         1.6849e-01,  9.1665e-03,  2.9251e-02,  5.3137e-02,  2.3092e-02,\n",
      "         1.7443e-01,  1.2236e-01,  6.8739e-02,  1.8878e-01,  4.8143e-02,\n",
      "         1.0529e-01,  5.3176e-02,  2.8194e-02,  4.5292e-02,  1.1953e-02,\n",
      "         1.4462e-01,  1.0484e-01,  1.7843e-01,  6.1222e-02,  6.2832e-02,\n",
      "         6.6576e-02,  1.9695e-01,  3.8580e-02,  1.5326e-01,  3.9051e-02,\n",
      "         5.2106e-02,  1.5796e-01,  8.9441e-02, -6.0227e-02,  1.5830e-03,\n",
      "         1.1930e-01,  7.4094e-02,  7.4535e-03, -2.3235e-03,  1.4657e-01,\n",
      "         9.6045e-02,  9.0926e-02,  2.6694e-02,  7.5614e-04,  9.3759e-03,\n",
      "         8.4982e-02,  1.3086e-01,  1.0728e-01,  1.3064e-01,  1.1889e-01,\n",
      "        -2.6282e-02,  4.8715e-02,  1.0595e-01,  7.0621e-02,  6.8555e-02,\n",
      "         1.2809e-01,  4.8504e-02,  4.2268e-02,  6.3003e-02,  6.8012e-02,\n",
      "         8.4302e-02,  1.1121e-01,  7.4018e-02, -1.3854e-02,  1.3481e-01,\n",
      "         2.6019e-02,  1.7185e-01,  9.0024e-02,  7.6185e-02,  5.5071e-03,\n",
      "         7.5954e-02,  1.0042e-01,  5.6631e-03,  1.1510e-02,  1.0361e-01,\n",
      "         1.2562e-01,  3.5685e-02,  1.6316e-03,  1.0759e-01,  1.5544e-01,\n",
      "         2.4144e-02,  1.1743e-02,  8.5615e-02,  7.7173e-02,  1.7808e-01,\n",
      "        -7.7273e-03,  4.0522e-02,  1.3188e-01,  8.3940e-03, -1.1928e-01,\n",
      "        -9.4709e-03,  7.8943e-02,  2.5476e-02, -6.4981e-03,  6.4920e-02,\n",
      "         9.7793e-02, -2.9308e-02,  5.4348e-02,  1.0350e-02, -1.0794e-01,\n",
      "        -8.8781e-02, -1.0956e-02, -1.4234e-02, -6.9202e-03, -8.3669e-02,\n",
      "         2.5782e-02, -1.2610e-01,  2.5655e-02, -9.2876e-02, -3.2792e-02,\n",
      "        -2.9426e-02,  1.2186e-01,  2.0137e-02,  3.8602e-02,  3.8073e-02,\n",
      "         3.3731e-02,  1.0191e-01, -1.6142e-02,  5.2464e-03, -8.2662e-02,\n",
      "        -7.0598e-02, -7.6153e-02, -4.7511e-03, -2.3625e-02,  3.7700e-02,\n",
      "        -4.0361e-03, -7.5573e-02, -1.9767e-02,  4.6302e-02,  5.7209e-03,\n",
      "        -6.2140e-02, -1.8325e-02, -6.7564e-02, -8.2021e-02, -1.0660e-01,\n",
      "         2.4335e-02,  1.4713e-02, -3.5989e-02, -4.6479e-02,  3.1790e-02,\n",
      "         2.0212e-02, -6.1954e-02, -1.0082e-01,  1.8239e-02,  6.4562e-02,\n",
      "         2.2449e-03, -2.1651e-02,  5.6700e-02,  1.1089e-01, -3.7793e-02,\n",
      "        -1.1332e-01, -7.7467e-02,  8.7663e-02, -4.6214e-02, -5.1378e-02,\n",
      "        -2.8729e-02, -1.8325e-02, -3.0632e-02,  9.6903e-02, -1.4809e-01,\n",
      "         8.8930e-02,  8.1682e-02,  1.1907e-01,  4.5768e-02, -5.2083e-03,\n",
      "         1.0093e-01, -1.0052e-01, -6.6204e-02,  4.1574e-02,  9.3603e-02,\n",
      "         7.9359e-02,  2.7120e-02,  8.6406e-04,  1.8593e-02,  1.0846e-01,\n",
      "        -9.8622e-02, -1.1548e-02, -2.0254e-02, -2.2726e-02,  2.6009e-02,\n",
      "         8.8766e-02,  5.1815e-02, -9.5470e-03,  1.1644e-02, -8.3227e-02,\n",
      "        -3.3121e-02,  2.1585e-02,  1.0577e-01, -6.8041e-03,  3.8070e-02,\n",
      "         1.2660e-02,  5.4808e-02, -7.6836e-02,  1.4095e-02, -5.6409e-02,\n",
      "         8.3679e-02, -1.4547e-02,  6.8388e-02,  3.2541e-02,  3.5224e-02,\n",
      "         7.0819e-03,  5.0703e-02, -2.2670e-02, -2.8325e-02,  4.3715e-02,\n",
      "        -1.0246e-03,  8.3935e-02,  3.0618e-02,  7.9657e-03, -1.8040e-02,\n",
      "        -8.4356e-02, -7.2203e-03, -1.2246e-01, -4.8897e-02, -9.8366e-02,\n",
      "         4.5999e-02, -4.2295e-02,  6.3149e-02, -2.0698e-02,  1.5238e-02,\n",
      "        -2.5329e-02,  7.4005e-02,  4.1779e-02, -9.8408e-02, -8.3847e-02,\n",
      "        -1.6007e-03,  1.1158e-01, -6.8634e-02, -8.1691e-02, -2.9169e-02,\n",
      "        -7.0003e-03,  1.0857e-01,  3.4911e-02, -2.5954e-02,  7.3321e-02,\n",
      "        -7.9419e-02, -2.6578e-02,  8.5152e-02,  6.5985e-02,  8.0173e-02,\n",
      "         1.1628e-01, -7.0339e-02,  3.1854e-02, -2.5168e-02,  1.0365e-01,\n",
      "         6.9614e-02,  2.1231e-02, -3.8597e-02,  9.0088e-02, -9.4254e-02,\n",
      "        -3.7794e-02,  5.7890e-02,  9.3018e-02,  6.3395e-02,  2.1695e-02,\n",
      "        -2.4539e-02, -1.4929e-02, -1.0473e-01, -7.2874e-02, -4.2908e-02,\n",
      "         1.0476e-01,  8.6406e-02,  4.6020e-02, -7.6081e-03, -6.6439e-04,\n",
      "         6.4319e-03, -3.7800e-02, -6.5459e-02,  9.4075e-02,  1.8538e-03,\n",
      "        -2.8067e-02,  9.7837e-02,  2.8094e-02, -4.5980e-02, -1.1944e-02,\n",
      "         2.8981e-02, -6.0443e-02,  1.0033e-01, -4.9666e-02, -4.9978e-02,\n",
      "        -1.1567e-02, -3.6722e-02, -4.6887e-02,  6.2109e-02, -2.5356e-02,\n",
      "        -1.6442e-02,  8.5611e-03, -1.1391e-02, -7.8370e-02,  7.2947e-02,\n",
      "        -3.8892e-02, -3.2632e-02,  4.9870e-02,  3.6589e-02, -3.4960e-02,\n",
      "         3.4705e-02,  9.6277e-02, -8.2164e-03,  2.6171e-03,  3.6527e-02,\n",
      "         2.4493e-03,  1.1143e-01, -9.3690e-02,  5.6907e-02, -4.5493e-03,\n",
      "         4.8660e-02,  1.1056e-01, -4.5069e-02, -2.0155e-02, -1.1197e-01,\n",
      "        -7.5437e-02, -3.9972e-02, -6.6106e-02, -1.0453e-01, -4.0713e-02,\n",
      "         7.9132e-02, -7.7119e-02,  4.1523e-02,  8.5884e-02,  6.7017e-04,\n",
      "        -2.1814e-03, -2.2073e-02, -2.4982e-02, -3.0216e-02, -7.1212e-02,\n",
      "        -1.0682e-01,  2.6503e-02, -7.8823e-02, -1.1784e-01,  3.6965e-02,\n",
      "        -1.5788e-04,  9.2903e-03,  9.9413e-02,  4.7910e-02,  1.0336e-01,\n",
      "         2.1657e-02,  3.0486e-02,  4.5880e-02, -1.5386e-02, -4.1543e-02,\n",
      "         3.7762e-02, -7.4407e-02,  1.1722e-02,  6.0604e-02,  1.2809e-01,\n",
      "         6.8710e-02,  1.4896e-01,  1.4218e-01,  1.7372e-01,  1.2702e-01,\n",
      "        -1.6150e-02,  1.8671e-02,  1.5478e-01,  4.0233e-02,  3.4521e-02,\n",
      "         1.6429e-01,  5.4725e-02,  2.8127e-03,  1.3638e-01,  4.1524e-02,\n",
      "         1.0622e-01,  5.4510e-02,  1.1978e-01, -3.0360e-02,  1.8318e-01,\n",
      "         1.6671e-01,  1.5731e-01,  8.4267e-02, -9.6968e-03,  8.1015e-02,\n",
      "         1.6719e-01,  5.3081e-02,  1.0805e-01,  7.7946e-02,  2.1160e-01,\n",
      "         1.7780e-01,  4.6310e-02,  3.7622e-02,  6.3667e-02,  7.1637e-02,\n",
      "         7.3340e-02,  4.8613e-02,  5.8433e-02,  5.2502e-02,  7.7058e-02,\n",
      "        -4.5047e-02,  3.5894e-02,  1.1860e-01,  5.8611e-02,  1.7928e-01,\n",
      "         7.1903e-02,  4.8285e-02,  6.4345e-02,  2.7179e-02,  1.2226e-01,\n",
      "         1.1459e-01,  1.4557e-01,  9.8073e-02,  1.2871e-01,  3.3588e-02,\n",
      "         7.9869e-02,  8.8881e-02,  8.5071e-02,  7.2436e-02, -2.0313e-02,\n",
      "         6.1773e-02,  5.8990e-02,  1.1148e-01,  2.2282e-02,  1.7585e-01,\n",
      "         1.2776e-01,  1.3729e-01,  1.1768e-01,  2.0617e-01, -9.5029e-03,\n",
      "         8.6872e-02,  2.2245e-02,  2.4530e-02,  1.2593e-01,  2.0443e-01,\n",
      "         9.1739e-02,  5.1442e-02,  4.8769e-02,  1.1710e-01,  4.9338e-02,\n",
      "         9.3699e-03,  2.3342e-02,  3.9936e-02,  6.4781e-02,  1.0956e-01,\n",
      "        -3.7131e-02,  1.3311e-01, -4.0062e-02,  7.5766e-02, -8.8909e-02,\n",
      "         1.1191e-01,  3.1502e-02,  1.1778e-01,  6.6257e-02,  7.3976e-02,\n",
      "         1.5096e-01, -5.2189e-04,  4.8601e-02, -2.2688e-02,  2.2273e-02,\n",
      "        -6.2681e-03,  3.0264e-02, -8.7538e-03,  8.8216e-02,  1.0781e-01,\n",
      "         5.5627e-02, -1.9363e-02,  9.3652e-02, -4.7078e-03,  1.2110e-01,\n",
      "         8.1083e-02,  7.7080e-02,  3.7247e-02, -7.5866e-03,  1.8045e-01,\n",
      "         1.8486e-01,  1.7686e-02,  2.3653e-02,  1.2876e-02,  2.0399e-01,\n",
      "         5.7911e-03,  1.5875e-01,  1.0256e-01,  4.0107e-02,  1.2162e-01,\n",
      "         2.7647e-02, -1.5928e-02], requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[ 0.0220, -0.0264, -0.0583,  ...,  0.0163, -0.0735, -0.0396],\n",
      "        [-0.0061, -0.0412, -0.0082,  ...,  0.0702, -0.0742, -0.0195],\n",
      "        [-0.0492, -0.0271, -0.0863,  ...,  0.0978,  0.0699,  0.0182],\n",
      "        ...,\n",
      "        [-0.0060,  0.0941,  0.0930,  ...,  0.0101,  0.2506,  0.2668],\n",
      "        [ 0.2101,  0.0861, -0.0419,  ..., -0.0147,  0.0641,  0.1477],\n",
      "        [-0.0194, -0.0083, -0.0866,  ...,  0.0230, -0.0551, -0.0087]],\n",
      "       requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([-5.8919e-03, -5.5742e-02,  2.5504e-02, -4.3477e-02, -1.0955e-02,\n",
      "        -1.4391e-02, -4.0914e-02,  3.5681e-05, -3.3668e-03,  3.3677e-02,\n",
      "         1.5780e-02,  2.8644e-03,  6.5813e-02, -7.0356e-02, -5.1918e-02,\n",
      "        -1.9205e-03, -2.6627e-02, -3.7485e-02, -2.9012e-02, -5.8696e-02],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [11520 x 100], m2: [300 x 512] at C:\\w\\1\\s\\tmp_conda_3.7_100118\\conda\\conda-bld\\pytorch_1579082551706\\work\\aten\\src\\TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cb2b31e7e28c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_pad_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_pad_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-3f761cde83f8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, tag_pad_idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ling380\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-ec3adf757313>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ling380\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ling380\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 559\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [11520 x 100], m2: [300 x 512] at C:\\w\\1\\s\\tmp_conda_3.7_100118\\conda\\conda-bld\\pytorch_1579082551706\\work\\aten\\src\\TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
